{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 루브릭\n",
    "\n",
    "## 프로젝트 평가 기준\n",
    "\n",
    "평가문항\t상세기준\n",
    "1. multiface detection을 위한 widerface 데이터셋의 전처리가 적절히 진행되었다.\n",
    "    - tfrecord 생성, augmentation, prior box 생성 등의 과정이 정상적으로 진행되었다.\n",
    "2. SSD 모델이 안정적으로 학습되어 multiface detection이 가능해졌다.\n",
    "    - inference를 통해 정확한 위치의 face bounding box를 detect한 결과이미지가 제출되었다.\n",
    "3. 이미지 속 다수의 얼굴에 스티커가 적용되었다.\n",
    "    - 이미지 속 다수의 얼굴의 적절한 위치에 스티커가 적용된 결과이미지가 제출되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIDER FACE 데이터셋\n",
    "## WIDER FACE 데이터셋\n",
    "- 오늘 우리가 Face Detection 모델의 학습을 위해 다루게 될 데이터셋은 바로 WIDER FACE 데이터셋입니다. 빠른 인퍼런스 타임을 위해 사용할 YOLO, SSD 같은 single stage model을 학습시키는 것은 흔히 COCO 데이터셋 같은 것이 사용되겠지만, 오늘 우리가 추구하는 [먼 거리에 흩어져 있는 다수의 사람 얼굴을 빠르게 detect하는 모델]을 만들기 위해서는 그에 적합하게 '보다 넓은 공간에 있는 다수의 사람이 등장하는 이미지 데이터셋'이 필요하겠죠? 아래 그림에서 확인할 수 있듯, WIDER FACE 데이터셋은 그런 용도로 활용하기에 적절해 보입니다.\n",
    "- WIDER FACE 데이터셋 홈페이지에 접속하여 아래 4개의 zip파일을 다운받아 ~/aiffel/face_detector/widerface에 저장합니다. 다운받아야 할 WIDER_xxx.zip 파일들은 구글드라이브에 올라가 있습니다. 아래 코드를 실행하면 다운로드가 자동으로 진행됩니다.\n",
    "- WIDER Face Dataset의 전체 이미지 개수와 얼굴 개수는 각각 얼마인가요?\n",
    "    - 32,203개의 이미지, 393,703개의 얼굴 데이터가 존재함\n",
    "- WIDER Face Dataset에서 train/validataion/test 의 구성비율은 각각 몇%씩인가요?\n",
    "    - train/validataion/test 의 구성비율이 각각 40%/10%/50%로 구성되어 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# datasets = [\n",
    "#    ('WIDER_train.zip', '0B6eKvaijfFUDQUUwd21EckhUbWs'),\n",
    "#    ('WIDER_val.zip', '0B6eKvaijfFUDd3dIRmpvSk8tLUk'),\n",
    "#    ('WIDER_test.zip', '0B6eKvaijfFUDbW4tdGpaYjgzZkU'),\n",
    "# ]\n",
    "\n",
    "# os.system('cd ~/aiffel/face_detector && mkdir widerface && cd widerface')\n",
    "\n",
    "# for FILENAME, FILEID in datasets:\n",
    "#    command = f\"wget --load-cookies /tmp/cookies.txt \\\"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILEID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\\\1\\\\n/p')&id={FILEID}\\\" -O widerface/{FILENAME} && rm -rf /tmp/cookies.txt\"\n",
    "#    os.system(command)\n",
    "\n",
    "# os.system('wget http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/bbx_annotation/wider_face_split.zip -O widerface/wider_face_split.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 전처리(1) 분석\n",
    "## WIDER FACE Bounding Box\n",
    "- 10개의 숫자로 이루어진 [face bounding box 좌표 등 상세정보]는 다음과 같은 의미를 가집니다.\n",
    "    - x0, y0, w, h, blur, expression, illumination, invalid, occlusion, pose\n",
    "- bounding box가 관련해서 가장 중요한 4개의 숫자는 왼쪽의 4개(좌상 꼭지점 X좌표, Y좌표, 너비, 높이)입니다.\n",
    "- 이 파일을 분석하는 코드는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box(data):\n",
    "    x0 = int(data[0])\n",
    "    y0 = int(data[1])\n",
    "    w = int(data[2])\n",
    "    h = int(data[3])\n",
    "    return x0, y0, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_widerface(config_path):\n",
    "    boxes_per_img = []\n",
    "    with open(config_path) as fp:\n",
    "        line = fp.readline()\n",
    "        cnt = 1\n",
    "        while line:\n",
    "            num_of_obj = int(fp.readline())\n",
    "            boxes = []\n",
    "            for i in range(num_of_obj):\n",
    "                obj_box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = get_box(obj_box)\n",
    "                if w == 0:\n",
    "                    # remove boxes with no width\n",
    "                    continue\n",
    "                if h == 0:\n",
    "                    # remove boxes with no height\n",
    "                    continue\n",
    "                # Because our network is outputting 7x7 grid then it's not worth processing images with more than\n",
    "                # 5 faces because it's highly probable they are close to each other.\n",
    "                # You could remove this filter if you decide to switch to larger grid (like 14x14)\n",
    "                # Don't worry about number of train data because even with this filter we have around 16k samples\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            if num_of_obj == 0:\n",
    "                obj_box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = get_box(obj_box)\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            boxes_per_img.append((line.strip(), boxes))\n",
    "            line = fp.readline()\n",
    "            cnt += 1\n",
    "\n",
    "    return boxes_per_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 메소드는 이미지별 bounding box 정보를 wider_face_train_bbx_gt.txt에서 파싱해서 리스트로 추출하는 것입니다. boudbing box 정보는 [x, y, w, h] 형태로 저장되어 있는데, 아래 코드를 통해 x_min, y_min, x_max, y_max 형태의 꼭지점 좌표 정보로 변환하여 출력해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_file):\n",
    "    image_string = tf.io.read_file(image_file)\n",
    "    try:\n",
    "        image_data = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        return 0, image_string, image_data\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        logging.info('{}: Invalid JPEG data or crop window'.format(image_file))\n",
    "        return 1, image_string, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh_to_voc(file_name, boxes, image_data):\n",
    "    shape = image_data.shape\n",
    "    image_info = {}\n",
    "    image_info['filename'] = file_name\n",
    "    image_info['width'] = shape[1]\n",
    "    image_info['height'] = shape[0]\n",
    "    image_info['depth'] = 3\n",
    "\n",
    "    difficult = []\n",
    "    classes = []\n",
    "    xmin, ymin, xmax, ymax = [], [], [], []\n",
    "\n",
    "    for box in boxes:\n",
    "        classes.append(1)\n",
    "        difficult.append(0)\n",
    "        xmin.append(box[0])\n",
    "        ymin.append(box[1])\n",
    "        xmax.append(box[0] + box[2])\n",
    "        ymax.append(box[1] + box[3])\n",
    "    image_info['class'] = classes\n",
    "    image_info['xmin'] = xmin\n",
    "    image_info['ymin'] = ymin\n",
    "    image_info['xmax'] = xmax\n",
    "    image_info['ymax'] = ymax\n",
    "    image_info['difficult'] = difficult\n",
    "\n",
    "    return image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "{'filename': '/home/aiffel0042/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_849.jpg', 'width': 1024, 'height': 1385, 'depth': 3, 'class': [1], 'xmin': [449], 'ymin': [330], 'xmax': [571], 'ymax': [479], 'difficult': [0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel0042/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_Parade_0_904.jpg', 'width': 1024, 'height': 1432, 'depth': 3, 'class': [1], 'xmin': [361], 'ymin': [98], 'xmax': [624], 'ymax': [437], 'difficult': [0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel0042/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_799.jpg', 'width': 1024, 'height': 768, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [78, 78, 113, 134, 163, 201, 182, 245, 304, 328, 389, 406, 436, 522, 643, 653, 793, 535, 29, 3, 20], 'ymin': [221, 238, 212, 260, 250, 218, 266, 279, 265, 295, 281, 293, 290, 328, 320, 224, 337, 311, 220, 232, 215], 'xmax': [85, 92, 124, 149, 177, 211, 197, 263, 320, 344, 406, 427, 458, 543, 666, 670, 816, 551, 40, 14, 32], 'ymax': [229, 255, 227, 275, 267, 230, 283, 294, 282, 315, 300, 314, 307, 346, 342, 249, 367, 328, 235, 247, 231], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel0042/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_117.jpg', 'width': 1024, 'height': 682, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [69, 227, 296, 353, 885, 819, 727, 598, 740], 'ymin': [359, 382, 305, 280, 377, 391, 342, 246, 308], 'xmax': [119, 283, 340, 393, 948, 853, 764, 631, 785], 'ymax': [395, 425, 331, 316, 418, 434, 373, 275, 341], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel0042/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_778.jpg', 'width': 1024, 'height': 852, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [27, 63, 64, 88, 231, 263, 367, 198, 293, 412, 441, 475, 510, 576, 577, 595, 570, 645, 719, 791, 884, 898, 945, 922, 743, 841, 980, 1001, 488, 586, 669, 744, 803, 294, 203], 'ymin': [226, 95, 63, 13, 1, 122, 68, 98, 161, 36, 23, 40, 23, 30, 71, 94, 126, 171, 98, 154, 97, 48, 89, 38, 71, 18, 56, 107, 2, 1, 1, 2, 3, 2, 0], 'xmax': [60, 79, 81, 104, 244, 277, 382, 213, 345, 426, 458, 489, 524, 592, 593, 611, 583, 697, 730, 845, 900, 913, 960, 937, 754, 857, 993, 1015, 500, 601, 681, 762, 821, 305, 216], 'ymax': [262, 114, 81, 28, 14, 142, 91, 116, 220, 56, 36, 61, 40, 45, 92, 114, 142, 229, 113, 203, 118, 69, 109, 54, 89, 34, 76, 120, 20, 18, 16, 17, 20, 12, 14], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "dataset_path = os.getenv('HOME')+'/aiffel/face_detector/widerface'\n",
    "anno_txt = 'wider_face_train_bbx_gt.txt'\n",
    "file_path = 'WIDER_train'\n",
    "for i, info in enumerate(parse_widerface(os.path.join(dataset_path, 'wider_face_split', anno_txt))):\n",
    "    print('--------------------')\n",
    "    image_file = os.path.join(dataset_path, file_path, 'images', info[0])\n",
    "    error, image_string, image_data = process_image(image_file)\n",
    "    boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "    print(boxes)\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어떤가요? 이미지별로 boxes 리스트에 담긴 bounding box 정보가 확인되시나요?\n",
    "이제 이 정보를 활용하여 텐서플로우 데이터셋을 생성해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 전처리(2) tf_example 생성\n",
    "## tfrecord 만들기\n",
    "- 오늘 우리가 다루게 될 대용량 데이터셋의 처리속도 향상을 위해서 전처리 작업을 통해 tfrecord 데이터셋으로 변환할 필요가 있습니다. 1개 데이터의 단위를 이루는 tf.train.Example 인스턴스를 생성하는 메소드는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(image_string, image_info_list):\n",
    "\n",
    "    for info in image_info_list:\n",
    "        filename = info['filename']\n",
    "        width = info['width']\n",
    "        height = info['height']\n",
    "        depth = info['depth']\n",
    "        classes = info['class']\n",
    "        xmin = info['xmin']\n",
    "        ymin = info['ymin']\n",
    "        xmax = info['xmax']\n",
    "        ymax = info['ymax']\n",
    "\n",
    "    if isinstance(image_string, type(tf.constant(0))):\n",
    "        encoded_image = [image_string.numpy()]\n",
    "    else:\n",
    "        encoded_image = [image_string]\n",
    "\n",
    "    base_name = [tf.compat.as_bytes(os.path.basename(filename))]\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'filename':tf.train.Feature(bytes_list=tf.train.BytesList(value=base_name)),\n",
    "        'height':tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'width':tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'classes':tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "        'x_mins':tf.train.Feature(float_list=tf.train.FloatList(value=xmin)),\n",
    "        'y_mins':tf.train.Feature(float_list=tf.train.FloatList(value=ymin)),\n",
    "        'x_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=xmax)),\n",
    "        'y_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=ymax)),\n",
    "        'image_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=encoded_image))\n",
    "    }))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 전처리를 위해 필요한 메소드들이 어느정도 갖추어졌습니다. 데이터셋의 이미지파일, 그리고 bounding box를 파싱한 정보를 모아 위의 make_example 메소드를 통해 만든 example을 serialize하여 바이너리 파일로 생성하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12880/12880 [00:30<00:00, 427.44it/s]\n",
      "100%|██████████| 3226/3226 [00:07<00:00, 415.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import tqdm\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "rootPath = os.getenv('HOME')+'/aiffel/face_detector'\n",
    "dataset_path = 'widerface'\n",
    "\n",
    "if not os.path.isdir(dataset_path):\n",
    "    logging.info('Please define valid dataset path.')\n",
    "else:\n",
    "    logging.info('Loading {}'.format(dataset_path))\n",
    "\n",
    "logging.info('Reading configuration...')\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    output_file = rootPath + '/dataset/train_mask.tfrecord' if split == 'train' else rootPath + '/dataset/val_mask.tfrecord'\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "\n",
    "        counter = 0\n",
    "        skipped = 0\n",
    "        anno_txt = 'wider_face_train_bbx_gt.txt' if split == 'train' else 'wider_face_val_bbx_gt.txt'\n",
    "        file_path = 'WIDER_train' if split == 'train' else 'WIDER_val'\n",
    "        for info in tqdm.tqdm(parse_widerface(os.path.join(rootPath, dataset_path, 'wider_face_split', anno_txt))):\n",
    "            image_file = os.path.join(rootPath, dataset_path, file_path, 'images', info[0])\n",
    "\n",
    "            error, image_string, image_data = process_image(image_file)\n",
    "            boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "\n",
    "            if not error:\n",
    "                tf_example = make_example(image_string, [boxes])\n",
    "\n",
    "                writer.write(tf_example.SerializeToString())\n",
    "                counter += 1\n",
    "\n",
    "            else:\n",
    "                skipped += 1\n",
    "                logging.info('Skipped {:d} of {:d} images.'.format(skipped, counter))\n",
    "\n",
    "    logging.info('Wrote {} images to {}'.format(counter, output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이전 스텝에서 여기까지의 전처리 과정을 tf_dataset_preprocess.py 에 정리해 두었습니다. 아래와 같이 위의 과정을 실행할 수 있습니다.\n",
    "- $ python tf_dataset_preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구현(1) priors box\n",
    "## SSD의 prior box\n",
    "- SSD 모델의 가장 중요한 특징 중 하나는 prior box(또는 anchor box)를 필요로 한다는 점입니다. \n",
    "- prior box란, object가 존재할 만한 다양한 크기의 box의 좌표 및 클래스 정보를 일정 개수만큼 미리 고정해 둔 것입니다. \n",
    "- ground truth에 해당하는 bounding box와의 IoU를 계산하여 일정 이상(0.5) 겹치는 prior box를 선택하는 방식이 RCNN 계열의 sliding window 방식보다 훨씬 속도가 빠르면서도 그와 유사한 정도의 정확도를 얻을 수 있다는 장점이 있습니다.\n",
    "- 이번 프로젝트에서 활용할 config 정보를 모아 아래와 같은 dict 구조로 정리하였습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'input_size': (240, 320),\n",
       " 'dataset_path': 'dataset/train_mask.tfrecord',\n",
       " 'val_path': 'dataset/val_mask.tfrecord',\n",
       " 'dataset_len': 12880,\n",
       " 'val_len': 3226,\n",
       " 'using_crop': True,\n",
       " 'using_bin': True,\n",
       " 'using_flip': True,\n",
       " 'using_distort': True,\n",
       " 'using_normalizing': True,\n",
       " 'labels_list': ['background', 'face'],\n",
       " 'min_sizes': [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],\n",
       " 'steps': [8, 16, 32, 64],\n",
       " 'match_thresh': 0.45,\n",
       " 'variances': [0.1, 0.2],\n",
       " 'clip': False,\n",
       " 'base_channel': 16,\n",
       " 'resume': False,\n",
       " 'epoch': 200,\n",
       " 'init_lr': 0.01,\n",
       " 'lr_decay_epoch': [50, 70],\n",
       " 'lr_rate': 0.1,\n",
       " 'warmup_epoch': 5,\n",
       " 'min_lr': 0.0001,\n",
       " 'weights_decay': 0.0005,\n",
       " 'momentum': 0.9,\n",
       " 'save_freq': 10,\n",
       " 'score_threshold': 0.6,\n",
       " 'nms_threshold': 0.4,\n",
       " 'max_number_keep': 200}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {\n",
    "    # general setting\n",
    "    \"batch_size\": 32,\n",
    "    \"input_size\": (240, 320),  # (h,w)\n",
    "\n",
    "    # training dataset\n",
    "    \"dataset_path\": 'dataset/train_mask.tfrecord',  # 'dataset/trainval_mask.tfrecord'\n",
    "    \"val_path\": 'dataset/val_mask.tfrecord',  #\n",
    "    \"dataset_len\": 12880,  # train 6115 , trainval 7954, number of training samples\n",
    "    \"val_len\": 3226,\n",
    "    \"using_crop\": True,\n",
    "    \"using_bin\": True,\n",
    "    \"using_flip\": True,\n",
    "    \"using_distort\": True,\n",
    "    \"using_normalizing\": True,\n",
    "    \"labels_list\": ['background', 'face'],  # xml annotation\n",
    "\n",
    "    # anchor setting\n",
    "    \"min_sizes\":[[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],\n",
    "    \"steps\": [8, 16, 32, 64],\n",
    "    \"match_thresh\": 0.45,\n",
    "    \"variances\": [0.1, 0.2],\n",
    "    \"clip\": False,\n",
    "\n",
    "    # network\n",
    "    \"base_channel\": 16,\n",
    "\n",
    "    # training setting\n",
    "    \"resume\": False,  # if False,training from scratch\n",
    "    \"epoch\": 200,\n",
    "    \"init_lr\": 1e-2,\n",
    "    \"lr_decay_epoch\": [50, 70],\n",
    "    \"lr_rate\": 0.1,\n",
    "    \"warmup_epoch\": 5,\n",
    "    \"min_lr\": 1e-4,\n",
    "\n",
    "    \"weights_decay\": 5e-4,\n",
    "    \"momentum\": 0.9,\n",
    "    \"save_freq\": 10, #frequency of save model weights\n",
    "\n",
    "    # inference\n",
    "    \"score_threshold\": 0.6,\n",
    "    \"nms_threshold\": 0.4,\n",
    "    \"max_number_keep\": 200\n",
    "}\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- config 중 prior(anchor) box 생성과 관련된 것들은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320)\n"
     ]
    }
   ],
   "source": [
    "image_sizes = cfg['input_size']\n",
    "min_sizes = cfg[\"min_sizes\"]\n",
    "steps = cfg[\"steps\"]\n",
    "clip = cfg[\"clip\"]\n",
    "\n",
    "if isinstance(image_sizes, int):\n",
    "    image_sizes = (image_sizes, image_sizes)\n",
    "elif isinstance(image_sizes, tuple):\n",
    "    image_sizes = image_sizes\n",
    "else:\n",
    "    raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "print(image_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 그림에서 보는 것처럼, prior box를 생성하기 위해서는 먼저 기준이 되는 feature map을 먼저 생성합니다. 그림에서는 8X8, 4X4의 예가 나오지만, 우리의 프로젝트에서는 아래와 같이 4가지 유형의 feature map을 생성하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 40], [15, 20], [8, 10], [4, 5]]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "for m in range(4):\n",
    "    if (steps[m] != pow(2, (m + 3))):\n",
    "        print(\"steps must be [8,16,32,64]\")\n",
    "        sys.exit()\n",
    "\n",
    "assert len(min_sizes) == len(steps), \"anchors number didn't match the feature map layer.\"\n",
    "\n",
    "feature_maps = [\n",
    "    [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "    for step in steps]\n",
    "\n",
    "feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 feature map별로 순회를 하면서 prior box 를 생성해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17680"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors = []\n",
    "num_box_fm_cell=[]\n",
    "for k, f in enumerate(feature_maps):\n",
    "    num_box_fm_cell.append(len(min_sizes[k]))\n",
    "    for i, j in product(range(f[0]), range(f[1])):\n",
    "        for min_size in min_sizes[k]:\n",
    "            if isinstance(min_size, int):\n",
    "                min_size = (min_size, min_size)\n",
    "            elif isinstance(min_size, tuple):\n",
    "                min_size=min_size\n",
    "            else:\n",
    "                raise Exception('Type error of min_sizes elements format,tuple or int. ')\n",
    "            s_kx = min_size[1] / image_sizes[1]\n",
    "            s_ky = min_size[0] / image_sizes[0]\n",
    "            cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "            cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "            anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "len(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4420, 4)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "priors = np.asarray(anchors).reshape([-1, 4])\n",
    "priors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0125    , 0.01666667, 0.03125   , 0.04166667],\n",
       "       [0.0125    , 0.01666667, 0.05      , 0.06666667],\n",
       "       [0.0125    , 0.01666667, 0.075     , 0.1       ],\n",
       "       ...,\n",
       "       [0.9       , 0.93333333, 0.4       , 0.53333333],\n",
       "       [0.9       , 0.93333333, 0.6       , 0.8       ],\n",
       "       [0.9       , 0.93333333, 0.8       , 1.06666667]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 스텝에 소개한 prior_box 생성 과정을 make_prior_box.py에 정리해 두었습니다. 아래와 같이 위의 과정을 실행할 수 있습니다.\n",
    "- $ cd ~/aiffel/face_detector && python make_prior_box.py\n",
    "- 아래는 이번 스텝에서 prior box를 생성하는 최종 메소드인 prior_box() 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_box(cfg,image_sizes=None):\n",
    "    \"\"\"prior box\"\"\"\n",
    "    if image_sizes is None:\n",
    "        image_sizes = cfg['input_size']\n",
    "    min_sizes=cfg[\"min_sizes\"]\n",
    "    steps=cfg[\"steps\"]\n",
    "    clip=cfg[\"clip\"]\n",
    "\n",
    "    if isinstance(image_sizes, int):\n",
    "        image_sizes = (image_sizes, image_sizes)\n",
    "    elif isinstance(image_sizes, tuple):\n",
    "        image_sizes = image_sizes\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    for m in range(4):\n",
    "        if (steps[m] != pow(2, (m + 3))):\n",
    "            print(\"steps must be [8,16,32,64]\")\n",
    "            sys.exit()\n",
    "\n",
    "    assert len(min_sizes) == len(steps), \"anchors number didn't match the feature map layer.\"\n",
    "\n",
    "    feature_maps = [\n",
    "        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "        for step in steps]\n",
    "\n",
    "    anchors = []\n",
    "    num_box_fm_cell=[]\n",
    "    for k, f in enumerate(feature_maps):\n",
    "        num_box_fm_cell.append(len(min_sizes[k]))\n",
    "        for i, j in product(range(f[0]), range(f[1])):\n",
    "            for min_size in min_sizes[k]:\n",
    "                if isinstance(min_size, int):\n",
    "                    min_size = (min_size, min_size)\n",
    "                elif isinstance(min_size, tuple):\n",
    "                    min_size=min_size\n",
    "                else:\n",
    "                    raise Exception('Type error of min_sizes elements format,tuple or int. ')\n",
    "                s_kx = min_size[1] / image_sizes[1]\n",
    "                s_ky = min_size[0] / image_sizes[0]\n",
    "                cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "                cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "                anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "    output = np.asarray(anchors).reshape([-1, 4])\n",
    "\n",
    "    if clip:\n",
    "        output = np.clip(output, 0, 1)\n",
    "    return output,num_box_fm_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구현(2) SSD\n",
    "## SSD model 빌드하기\n",
    "- 그럼 본격적으로 SSD 모델을 생성해 보겠습니다.\n",
    "- 우선은 SSD 모델 내부에서 사용하는 레이어들을 아래와 같이 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _conv_block(inputs, filters, kernel=(3, 3), strides=(1, 1), use_bn=True, padding=None, block_id=None):\n",
    "    \"\"\"Adds an initial convolution layer (with batch normalization and relu).\n",
    "    # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    if block_id is None:\n",
    "        block_id = (tf.keras.backend.get_uid())\n",
    "\n",
    "    if strides == (2, 2):\n",
    "        x = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='valid',\n",
    "                                   use_bias=False if use_bn else True,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='same',\n",
    "                                   use_bias=False if use_bn else True,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(inputs)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_bn_%d' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_relu_%d' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _depthwise_conv_block(inputs, pointwise_conv_filters,\n",
    "                          depth_multiplier=1, strides=(1, 1), use_bn=True, block_id=None):\n",
    "    \"\"\"Adds a depthwise convolution block.\n",
    "        # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    if block_id is None:\n",
    "        block_id = (tf.keras.backend.get_uid())\n",
    "\n",
    "    if strides == (1, 1):\n",
    "        x = inputs\n",
    "    else:\n",
    "        x = tf.keras.layers.ZeroPadding2D(((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "\n",
    "    x = tf.keras.layers.DepthwiseConv2D((3, 3),\n",
    "                                        padding='same' if strides == (1, 1) else 'valid',\n",
    "                                        depth_multiplier=depth_multiplier,\n",
    "                                        strides=strides,\n",
    "                                        use_bias=False if use_bn else True,\n",
    "                                        name='conv_dw_%d' % block_id)(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_dw_%d_bn' % block_id)(x)\n",
    "    x = tf.keras.layers.ReLU(name='conv_dw_%d_relu' % block_id)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(pointwise_conv_filters, (1, 1),\n",
    "                               padding='same',\n",
    "                               use_bias=False if use_bn else True,\n",
    "                               strides=(1, 1),\n",
    "                               name='conv_pw_%d' % block_id)(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_pw_%d_bn' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_pw_%d_relu' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _branch_block(input, filters):\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(input)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv2D(filters * 2, kernel_size=(3, 3), padding='same')(input)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(axis=-1)([x, x1])\n",
    "\n",
    "    return tf.keras.layers.ReLU()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_head_block(inputs, filters, strides=(1, 1), block_id=None):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_heads(x, idx, num_class, num_cell):\n",
    "    \"\"\" Compute outputs of classification and regression heads\n",
    "    Args:\n",
    "        x: the input feature map\n",
    "        idx: index of the head layer\n",
    "    Returns:\n",
    "        conf: output of the idx-th classification head\n",
    "        loc: output of the idx-th regression head\n",
    "    \"\"\"\n",
    "    conf = _create_head_block(inputs=x, filters=num_cell[idx] * num_class)\n",
    "    conf = tf.keras.layers.Reshape((-1, num_class))(conf)\n",
    "    loc = _create_head_block(inputs=x, filters=num_cell[idx] * 4)\n",
    "    loc = tf.keras.layers.Reshape((-1, 4))(loc)\n",
    "\n",
    "    return conf, loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 레이어들이 준비되었습니다. 이제 본격적으로 SSD model을 생성해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SsdModel(cfg, num_cell, training=False, name='ssd_model'):\n",
    "    image_sizes = cfg['input_size']   if training else None\n",
    "    if isinstance(image_sizes, int):\n",
    "        image_sizes = (image_sizes, image_sizes)\n",
    "    elif isinstance(image_sizes, tuple):\n",
    "        image_sizes = image_sizes\n",
    "    elif image_sizes == None:\n",
    "        image_sizes = (None, None)\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    base_channel = cfg[\"base_channel\"]\n",
    "    num_class = len(cfg['labels_list'])\n",
    "\n",
    "    x = inputs = tf.keras.layers.Input(shape=[image_sizes[0], image_sizes[1], 3], name='input_image')\n",
    "\n",
    "    x = _conv_block(x, base_channel, strides=(2, 2))  # 120*160*16\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 2, strides=(2, 2))  # 60*80\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(2, 2))  # 30*40\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x1 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _conv_block(x, base_channel * 8, strides=(2, 2))  # 15*20\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x2 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))  # 8*10\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(1, 1))\n",
    "    x3 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))  # 4*5\n",
    "    x4 = _branch_block(x, base_channel)\n",
    "\n",
    "    extra_layers = [x1, x2, x3, x4]\n",
    "\n",
    "    confs = []\n",
    "    locs = []\n",
    "\n",
    "    head_idx = 0\n",
    "    assert len(extra_layers) == len(num_cell)\n",
    "    for layer in extra_layers:\n",
    "        conf, loc = _compute_heads(layer, head_idx, num_class, num_cell)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "\n",
    "        head_idx += 1\n",
    "\n",
    "    confs = tf.keras.layers.Concatenate(axis=1, name=\"face_classes\")(confs)\n",
    "    locs = tf.keras.layers.Concatenate(axis=1, name=\"face_boxes\")(locs)\n",
    "\n",
    "    predictions = tf.keras.layers.Concatenate(axis=2, name='predictions')([locs, confs])\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "Model: \"ssd_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_29 (ZeroPadding2D)     (None, None, None, 3 0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_29 (Conv2D)                (None, None, None, 1 432         conv_pad_29[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_29 (BatchNormalization) (None, None, None, 1 64          conv_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_29 (ReLU)             (None, None, None, 1 0           conv_bn_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_30 (Conv2D)                (None, None, None, 3 4608        conv_relu_29[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_30 (BatchNormalization) (None, None, None, 3 128         conv_30[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_30 (ReLU)             (None, None, None, 3 0           conv_bn_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_31 (ZeroPadding2D)     (None, None, None, 3 0           conv_relu_30[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_31 (Conv2D)                (None, None, None, 3 9216        conv_pad_31[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_31 (BatchNormalization) (None, None, None, 3 128         conv_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_31 (ReLU)             (None, None, None, 3 0           conv_bn_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_32 (Conv2D)                (None, None, None, 3 9216        conv_relu_31[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_32 (BatchNormalization) (None, None, None, 3 128         conv_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_32 (ReLU)             (None, None, None, 3 0           conv_bn_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_33 (ZeroPadding2D)     (None, None, None, 3 0           conv_relu_32[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_33 (Conv2D)                (None, None, None, 6 18432       conv_pad_33[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_33 (BatchNormalization) (None, None, None, 6 256         conv_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_33 (ReLU)             (None, None, None, 6 0           conv_bn_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_34 (Conv2D)                (None, None, None, 6 36864       conv_relu_33[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_34 (BatchNormalization) (None, None, None, 6 256         conv_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_34 (ReLU)             (None, None, None, 6 0           conv_bn_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_35 (Conv2D)                (None, None, None, 6 36864       conv_relu_34[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_35 (BatchNormalization) (None, None, None, 6 256         conv_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_35 (ReLU)             (None, None, None, 6 0           conv_bn_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_36 (Conv2D)                (None, None, None, 6 36864       conv_relu_35[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_36 (BatchNormalization) (None, None, None, 6 256         conv_36[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_36 (ReLU)             (None, None, None, 6 0           conv_bn_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_37 (ZeroPadding2D)     (None, None, None, 6 0           conv_relu_36[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_37 (Conv2D)                (None, None, None, 1 73728       conv_pad_37[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_37 (BatchNormalization) (None, None, None, 1 512         conv_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_37 (ReLU)             (None, None, None, 1 0           conv_bn_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_38 (Conv2D)                (None, None, None, 1 147456      conv_relu_37[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_38 (BatchNormalization) (None, None, None, 1 512         conv_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_38 (ReLU)             (None, None, None, 1 0           conv_bn_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_39 (Conv2D)                (None, None, None, 1 147456      conv_relu_38[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_39 (BatchNormalization) (None, None, None, 1 512         conv_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_39 (ReLU)             (None, None, None, 1 0           conv_bn_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_40 (ZeroPadding2D)     (None, None, None, 1 0           conv_relu_39[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_40 (DepthwiseConv2D)    (None, None, None, 1 1152        conv_pad_40[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_40_bn (BatchNormalizati (None, None, None, 1 512         conv_dw_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_40_relu (ReLU)          (None, None, None, 1 0           conv_dw_40_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_40 (Conv2D)             (None, None, None, 2 32768       conv_dw_40_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_40_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_40_relu (ReLU)          (None, None, None, 2 0           conv_pw_40_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_41 (DepthwiseConv2D)    (None, None, None, 2 2304        conv_pw_40_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_41_bn (BatchNormalizati (None, None, None, 2 1024        conv_dw_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_41_relu (ReLU)          (None, None, None, 2 0           conv_dw_41_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_41 (Conv2D)             (None, None, None, 2 65536       conv_dw_41_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_41_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_41_relu (ReLU)          (None, None, None, 2 0           conv_pw_41_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_42 (ZeroPadding2D)     (None, None, None, 2 0           conv_pw_41_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_42 (DepthwiseConv2D)    (None, None, None, 2 2304        conv_pad_42[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_42_bn (BatchNormalizati (None, None, None, 2 1024        conv_dw_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_42_relu (ReLU)          (None, None, None, 2 0           conv_dw_42_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_42 (Conv2D)             (None, None, None, 2 65536       conv_dw_42_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_42_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_42_relu (ReLU)          (None, None, None, 2 0           conv_pw_42_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, None, None, 1 9232        conv_relu_36[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, None, None, 1 18448       conv_relu_39[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, None, None, 1 36880       conv_pw_41_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, None, None, 1 36880       conv_pw_42_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, None, None, 1 0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, None, None, 1 0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, None, None, 1 0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, None, None, 1 0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, None, None, 1 2320        leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, None, None, 3 18464       conv_relu_36[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, None, None, 1 2320        leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, None, None, 3 36896       conv_relu_39[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, None, None, 1 2320        leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, None, None, 3 73760       conv_pw_41_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, None, None, 1 2320        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, None, None, 3 73760       conv_pw_42_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, None, None, 4 0           conv2d_41[0][0]                  \n",
      "                                                                 conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, None, None, 4 0           conv2d_44[0][0]                  \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, None, None, 4 0           conv2d_47[0][0]                  \n",
      "                                                                 conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, None, None, 4 0           conv2d_50[0][0]                  \n",
      "                                                                 conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, None, None, 4 0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, None, None, 4 0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, None, None, 4 0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, None, None, 4 0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, None, None, 1 5196        re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, None, None, 8 3464        re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, None, None, 8 3464        re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, None, None, 1 5196        re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, None, None, 6 2598        re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, None, None, 4 1732        re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, None, None, 4 1732        re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, None, None, 6 2598        re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, None, 4)      0           conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_19 (Reshape)            (None, None, 4)      0           conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_21 (Reshape)            (None, None, 4)      0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_23 (Reshape)            (None, None, 4)      0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, None, 2)      0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, None, 2)      0           conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, None, 2)      0           conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_22 (Reshape)            (None, None, 2)      0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_boxes (Concatenate)        (None, None, 4)      0           reshape_17[0][0]                 \n",
      "                                                                 reshape_19[0][0]                 \n",
      "                                                                 reshape_21[0][0]                 \n",
      "                                                                 reshape_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "face_classes (Concatenate)      (None, None, 2)      0           reshape_16[0][0]                 \n",
      "                                                                 reshape_18[0][0]                 \n",
      "                                                                 reshape_20[0][0]                 \n",
      "                                                                 reshape_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, None, 6)      0           face_boxes[0][0]                 \n",
      "                                                                 face_classes[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,038,956\n",
      "Trainable params: 1,034,636\n",
      "Non-trainable params: 4,320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model = SsdModel(cfg, num_cell=[3, 2, 2, 3], training=False)\n",
    "print(len(model.layers))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 스텝에 소개한 SSD model 생성 과정을 tf_build_ssd_model.py에 정리해 두었습니다. 아래와 같이 위의 과정을 실행할 수 있습니다.\n",
    "\n",
    "- $ cd ~/aiffel/face_detector && python tf_build_ssd_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18-6. 모델 학습(1) Augmentation, Prior 적용\n",
    "## augmentation\n",
    "- 모델까지 구성완료하였습니다. 그러나 본격적으로 train을 진행하기 전에 아직 몇가지가 더 남아있습니다.\n",
    "\n",
    "- 이전 스텝에서 구성한 tfrecordset 형태의 데이터셋은 아직 Data augmentation이 적용되지 않았습니다. Object detection에서 사용하는 다양한 augmentation 기법을 적용해 주면 좀더 성능향상을 기대할 수도 있을 것입니다.\n",
    "\n",
    "- 아래는 augmentation을 위해 tf.data.TFRecordDataset.map() 내에서 호출할 메소드들입니다.\n",
    "\n",
    "    - _crop\n",
    "    - _pad_to_square\n",
    "    - _resize\n",
    "    - _flip\n",
    "    - _distort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop(img, labels, max_loop=250):\n",
    "    shape = tf.shape(img)\n",
    "\n",
    "    def matrix_iof(a, b):\n",
    "        \"\"\"\n",
    "        return iof of a and b, numpy version for data augenmentation\n",
    "        \"\"\"\n",
    "        lt = tf.math.maximum(a[:, tf.newaxis, :2], b[:, :2])\n",
    "        rb = tf.math.minimum(a[:, tf.newaxis, 2:], b[:, 2:])\n",
    "\n",
    "        area_i = tf.math.reduce_prod(rb - lt, axis=2) * \\\n",
    "            tf.cast(tf.reduce_all(lt < rb, axis=2), tf.float32)\n",
    "        area_a = tf.math.reduce_prod(a[:, 2:] - a[:, :2], axis=1)\n",
    "        return area_i / tf.math.maximum(area_a[:, tf.newaxis], 1)\n",
    "\n",
    "    def crop_loop_body(i, img, labels):\n",
    "        valid_crop = tf.constant(1, tf.int32)\n",
    "\n",
    "        pre_scale = tf.constant([0.3, 0.45, 0.6, 0.8, 1.0], dtype=tf.float32)\n",
    "        scale = pre_scale[tf.random.uniform([], 0, 5, dtype=tf.int32)]\n",
    "        short_side = tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)\n",
    "        h = w = tf.cast(scale * short_side, tf.int32)\n",
    "        h_offset = tf.random.uniform([], 0, shape[0] - h + 1, dtype=tf.int32)\n",
    "        w_offset = tf.random.uniform([], 0, shape[1] - w + 1, dtype=tf.int32)\n",
    "        roi = tf.stack([w_offset, h_offset, w_offset + w, h_offset + h])\n",
    "        roi = tf.cast(roi, tf.float32)\n",
    "\n",
    "\n",
    "        value = matrix_iof(labels[:, :4], roi[tf.newaxis])\n",
    "        valid_crop = tf.cond(tf.math.reduce_any(value >= 1),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        centers = (labels[:, :2] + labels[:, 2:4]) / 2\n",
    "        mask_a = tf.reduce_all(\n",
    "            tf.math.logical_and(roi[:2] < centers, centers < roi[2:]),\n",
    "            axis=1)\n",
    "        labels_t = tf.boolean_mask(labels, mask_a)\n",
    "        valid_crop = tf.cond(tf.reduce_any(mask_a),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        img_t = img[h_offset:h_offset + h, w_offset:w_offset + w, :]\n",
    "        h_offset = tf.cast(h_offset, tf.float32)\n",
    "        w_offset = tf.cast(w_offset, tf.float32)\n",
    "        labels_t = tf.stack(\n",
    "            [labels_t[:, 0] - w_offset,  labels_t[:, 1] - h_offset,\n",
    "             labels_t[:, 2] - w_offset,  labels_t[:, 3] - h_offset,\n",
    "             labels_t[:, 4]], axis=1)\n",
    "\n",
    "        return tf.cond(valid_crop == 1,\n",
    "                       lambda: (max_loop, img_t, labels_t),\n",
    "                       lambda: (i + 1, img, labels))\n",
    "\n",
    "    _, img, labels = tf.while_loop(\n",
    "        lambda i, img, labels: tf.less(i, max_loop),\n",
    "        crop_loop_body,\n",
    "        [tf.constant(-1), img, labels],\n",
    "        shape_invariants=[tf.TensorShape([]),\n",
    "                          tf.TensorShape([None, None, 3]),\n",
    "                          tf.TensorShape([None, 5])])\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_to_square(img):\n",
    "    height = tf.shape(img)[0]\n",
    "    width = tf.shape(img)[1]\n",
    "\n",
    "    def pad_h():\n",
    "        img_pad_h = tf.ones([width - height, width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_h], axis=0)\n",
    "\n",
    "    def pad_w():\n",
    "        img_pad_w = tf.ones([height, height - width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_w], axis=1)\n",
    "\n",
    "    img = tf.case([(tf.greater(height, width), pad_w),\n",
    "                   (tf.less(height, width), pad_h)], default=lambda: img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resize(img, labels, img_dim):\n",
    "    ''' # resize and boxes coordinate to percent'''\n",
    "    w_f = tf.cast(tf.shape(img)[1], tf.float32)\n",
    "    h_f = tf.cast(tf.shape(img)[0], tf.float32)\n",
    "    locs = tf.stack([labels[:, 0] / w_f,  labels[:, 1] / h_f,\n",
    "                     labels[:, 2] / w_f,  labels[:, 3] / h_f] ,axis=1)\n",
    "    locs = tf.clip_by_value(locs, 0, 1.0)\n",
    "    labels = tf.concat([locs, labels[:, 4][:, tf.newaxis]], axis=1)\n",
    "\n",
    "    resize_case = tf.random.uniform([], 0, 5, dtype=tf.int32)\n",
    "    if isinstance(img_dim, int):\n",
    "        img_dim = (img_dim, img_dim)\n",
    "    elif isinstance(img_dim,tuple):\n",
    "        img_dim = img_dim\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    def resize(method):\n",
    "        def _resize():\n",
    "            #　size h,w\n",
    "            return tf.image.resize(img, [img_dim[0], img_dim[1]], method=method, antialias=True)\n",
    "        return _resize\n",
    "\n",
    "    img = tf.case([(tf.equal(resize_case, 0), resize('bicubic')),\n",
    "                   (tf.equal(resize_case, 1), resize('area')),\n",
    "                   (tf.equal(resize_case, 2), resize('nearest')),\n",
    "                   (tf.equal(resize_case, 3), resize('lanczos3'))],\n",
    "                  default=resize('bilinear'))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flip(img, labels):\n",
    "    flip_case = tf.random.uniform([], 0, 2, dtype=tf.int32)\n",
    "\n",
    "    def flip_func():\n",
    "        flip_img = tf.image.flip_left_right(img)\n",
    "        flip_labels = tf.stack([1 - labels[:, 2],  labels[:, 1],\n",
    "                                1 - labels[:, 0],  labels[:, 3],\n",
    "                                labels[:, 4]], axis=1)\n",
    "\n",
    "        return flip_img, flip_labels\n",
    "\n",
    "    img, labels = tf.case([(tf.equal(flip_case, 0), flip_func)],default=lambda: (img, labels))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distort(img):\n",
    "    img = tf.image.random_brightness(img, 0.4)\n",
    "    img = tf.image.random_contrast(img, 0.5, 1.5)\n",
    "    img = tf.image.random_saturation(img, 0.5, 1.5)\n",
    "    img = tf.image.random_hue(img, 0.1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior box 적용\n",
    "- SSD 모델의 특이점 중 하나가 prior box를 사용한다는 점을 이미 앞에서 설명하였습니다. prior box 정보는 데이터셋에 반영되어야 합니다. 아래 메소드들은 prior box와 bounding box 사이의 IoU, 다른 말로 jaccard index를 측정하기 위한 것입니다. 이후 이 메소드를 활용해 어떻게 데이터셋을 추가로 가공하는지 살펴봅시다.\n",
    "\n",
    "- (참고) 자카드 거리, 자카드 지수 https://rfriend.tistory.com/318\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _intersect(box_a, box_b):\n",
    "    \"\"\" We resize both tensors to [A,B,2]:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = tf.shape(box_a)[0]\n",
    "    B = tf.shape(box_b)[0]\n",
    "    max_xy = tf.minimum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, 2:], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, 2:], 0), [A, B, 2]))\n",
    "    min_xy = tf.maximum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, :2], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, :2], 0), [A, B, 2]))\n",
    "    inter = tf.clip_by_value(max_xy - min_xy, 0.0, 512.0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    inter = _intersect(box_a, box_b)\n",
    "    area_a = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), 1),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    area_b = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), 0),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- jaccard index를 계산하는 메소드가 준비되었습니다. 아래 encode_tf는 이를 이용해서 tfrecord 데이터셋의 라벨을 가공하는 메소드입니다. 내용을 정리하면 다음과 같습니다.\n",
    "\n",
    "    - jaccard 메소드를 이용해 label의 ground truth bbox와 가장 overlap 비율이 높은 matched prior를 구한다.\n",
    "    - _encode_bbox 메소드를 통해 bbox의 scale을 동일하게 보정한다.\n",
    "    - 전체 prior box에 대해 일정 threshold 이상 overlap되는 ground truth bounding box 존재 여부(positive/negative)를 concat하여 새로운 label로 업데이트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_bbox(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth\n",
    "    boxes we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of prior boxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n",
    "\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = tf.math.log(g_wh) / variances[1]\n",
    "\n",
    "    # return target for smooth_l1_loss\n",
    "    return tf.concat([g_cxcy, g_wh], 1)  # [num_priors,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tf(labels, priors, match_thresh, variances=None):\n",
    "    \"\"\"tensorflow encoding\"\"\"\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "\n",
    "    priors = tf.cast(priors, tf.float32)\n",
    "    bbox = labels[:, :4]\n",
    "    conf = labels[:, -1]\n",
    "\n",
    "    # jaccard index\n",
    "    overlaps = _jaccard(bbox, priors)\n",
    "    best_prior_overlap = tf.reduce_max(overlaps, 1)\n",
    "    best_prior_idx = tf.argmax(overlaps, 1, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.reduce_max(overlaps, 0)\n",
    "    best_truth_idx = tf.argmax(overlaps, 0, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.tensor_scatter_nd_update(\n",
    "        best_truth_overlap, tf.expand_dims(best_prior_idx, 1),\n",
    "        tf.ones_like(best_prior_idx, tf.float32) * 2.)\n",
    "    best_truth_idx = tf.tensor_scatter_nd_update(\n",
    "        best_truth_idx, tf.expand_dims(best_prior_idx, 1),\n",
    "        tf.range(tf.size(best_prior_idx), dtype=tf.int32))\n",
    "\n",
    "    # Scale Ground-Truth Boxes\n",
    "    matches_bbox = tf.gather(bbox, best_truth_idx)  # [num_priors, 4]\n",
    "    loc_t = _encode_bbox(matches_bbox, priors, variances)\n",
    "    conf_t = tf.gather(conf, best_truth_idx)  # [num_priors]\n",
    "    conf_t = tf.where(tf.less(best_truth_overlap, match_thresh), tf.zeros_like(conf_t), conf_t)\n",
    "\n",
    "    return tf.concat([loc_t, conf_t[..., tf.newaxis]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_dataset\n",
    "- 위에서 구현한 두가지 메소드를 이전 스텝에서 생성한 tfrecord 데이터셋에 적용하여 SSD 학습을 위한 데이터셋을 생성하는 최종 메소드인 load_dataset 을 구현합니다.\n",
    "\n",
    "    - _transform_data : aumemtation과 prior box label을 적용하여 기존의 dataset을 변환하는 메소드\n",
    "    - _parse_tfrecord : tfrecord 에 _transform_data를 적용하는 함수 클로저 생성\n",
    "    - load_tfrecord_dataset : tf.data.TFRecordDataset.map()에 _parse_tfrecord을 적용하는 실제 데이터셋 변환 메인 메소드\n",
    "    - load_dataset : load_tfrecord_dataset을 통해 train, validation 데이터셋을 생성하는 최종 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_data(img_dim, using_crop,using_flip, using_distort, using_encoding,using_normalizing, priors,\n",
    "                    match_thresh,  variances):\n",
    "    def transform_data(img, labels):\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        if using_crop:\n",
    "        # randomly crop\n",
    "            img, labels = _crop(img, labels)\n",
    "\n",
    "            # padding to square\n",
    "            img = _pad_to_square(img)\n",
    "\n",
    "        # resize and boxes coordinate to percent\n",
    "        img, labels = _resize(img, labels, img_dim)\n",
    "\n",
    "        # randomly left-right flip\n",
    "        if using_flip:\n",
    "            img, labels = _flip(img, labels)\n",
    "\n",
    "        # distort\n",
    "        if using_distort:\n",
    "            img = _distort(img)\n",
    "\n",
    "        # encode labels to feature targets\n",
    "        if using_encoding:\n",
    "            labels = encode_tf(labels=labels, priors=priors, match_thresh=match_thresh, variances=variances)\n",
    "        if using_normalizing:\n",
    "            img=(img/255.0-0.5)/1.0\n",
    "\n",
    "        return img, labels\n",
    "    return transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_tfrecord(img_dim,using_crop, using_flip, using_distort,\n",
    "                    using_encoding, using_normalizing,priors, match_thresh,  variances):\n",
    "    def parse_tfrecord(tfrecord):\n",
    "        features = {\n",
    "            'filename': tf.io.FixedLenFeature([], tf.string),\n",
    "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'classes': tf.io.VarLenFeature(tf.int64),\n",
    "            'x_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'x_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'difficult':tf.io.VarLenFeature(tf.int64),\n",
    "            'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "           }\n",
    "\n",
    "        parsed_example = tf.io.parse_single_example(tfrecord, features)\n",
    "        img = tf.image.decode_jpeg(parsed_example['image_raw'], channels=3)\n",
    "\n",
    "        width = tf.cast(parsed_example['width'], tf.float32)\n",
    "        height = tf.cast(parsed_example['height'], tf.float32)\n",
    "\n",
    "        labels = tf.sparse.to_dense(parsed_example['classes'])\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "\n",
    "        labels = tf.stack(\n",
    "            [tf.sparse.to_dense(parsed_example['x_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['y_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['x_maxes']),\n",
    "             tf.sparse.to_dense(parsed_example['y_maxes']),labels], axis=1)\n",
    "\n",
    "        img, labels = _transform_data(\n",
    "            img_dim, using_crop,using_flip, using_distort, using_encoding, using_normalizing,priors,\n",
    "            match_thresh,  variances)(img, labels)\n",
    "\n",
    "        return img, labels\n",
    "    return parse_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tfrecord_dataset(tfrecord_name, batch_size, img_dim,\n",
    "                          using_crop=True,using_flip=True, using_distort=True,\n",
    "                          using_encoding=True, using_normalizing=True,\n",
    "                          priors=None, match_thresh=0.45,variances=None,\n",
    "                          shuffle=True, repeat=True,buffer_size=10240):\n",
    "\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "\n",
    "    \"\"\"load dataset from tfrecord\"\"\"\n",
    "    if not using_encoding:\n",
    "        assert batch_size == 1\n",
    "    else:\n",
    "        assert priors is not None\n",
    "\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\n",
    "    raw_dataset = raw_dataset.cache()\n",
    "    if repeat:\n",
    "        raw_dataset = raw_dataset.repeat()\n",
    "    if shuffle:\n",
    "        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "\n",
    "    dataset = raw_dataset.map(\n",
    "        _parse_tfrecord(img_dim, using_crop, using_flip, using_distort,\n",
    "                        using_encoding, using_normalizing,priors, match_thresh,  variances),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(\n",
    "        buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(cfg, priors, shuffle=True, buffer_size=10240,train=True):\n",
    "    \"\"\"load dataset\"\"\"\n",
    "    global dataset\n",
    "    if train:\n",
    "        logging.info(\"load train dataset from {}\".format(cfg['dataset_path']))\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=os.path.join(rootPath, cfg['dataset_path']),\n",
    "            batch_size=cfg['batch_size'],\n",
    "            img_dim=cfg['input_size'],\n",
    "            using_crop=cfg['using_crop'],\n",
    "            using_flip=cfg['using_flip'],\n",
    "            using_distort=cfg['using_distort'],\n",
    "            using_encoding=True,\n",
    "            using_normalizing=cfg['using_normalizing'],\n",
    "            priors=priors,\n",
    "            match_thresh=cfg['match_thresh'],\n",
    "            variances=cfg['variances'],\n",
    "            shuffle=shuffle,\n",
    "            repeat=True,\n",
    "            buffer_size=buffer_size)\n",
    "    else:\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=os.path.join(rootPath, cfg['val_path']),\n",
    "            batch_size=cfg['batch_size'],\n",
    "            img_dim=cfg['input_size'],\n",
    "            using_crop=False,\n",
    "            using_flip=False,\n",
    "            using_distort=False,\n",
    "            using_encoding=True,\n",
    "            using_normalizing=True,\n",
    "            priors=priors,\n",
    "            match_thresh=cfg['match_thresh'],\n",
    "            variances=cfg['variances'],\n",
    "            shuffle=shuffle,\n",
    "            repeat=False,\n",
    "            buffer_size=buffer_size)\n",
    "        logging.info(\"load validation dataset from {}\".format(cfg['val_path']))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 스텝에 소개한 load_dataset 메소드 구현체를 tf_dataloader.py에 정리해 두었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습(2) Train\n",
    "## Learning rate scheduler\n",
    "- 본격적으로 train에 들어가기 전에 2가지 더 준비해야 할 게 있습니다. \n",
    "- 그중 하나는 Learning rate scheduler 입니다. \n",
    "- 이번에는 구간별로 learning rate가 일정하게 유지하면서 감소하는 PiecewiseConstantDecay를 상속받아, 초기시점에 WarmUp부분을 추가한 PiecewiseConstantWarmUpDecay를 활용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiecewiseConstantWarmUpDecay(\n",
    "        tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"A LearningRateSchedule wiht warm up schedule.\n",
    "    Modified from tf.keras.optimizers.schedules.PiecewiseConstantDecay\"\"\"\n",
    "\n",
    "    def __init__(self, boundaries, values, warmup_steps, min_lr,\n",
    "                 name=None):\n",
    "        super(PiecewiseConstantWarmUpDecay, self).__init__()\n",
    "\n",
    "        if len(boundaries) != len(values) - 1:\n",
    "            raise ValueError(\n",
    "                    \"The length of boundaries should be 1 less than the\"\n",
    "                    \"length of values\")\n",
    "\n",
    "        self.boundaries = boundaries\n",
    "        self.values = values\n",
    "        self.name = name\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"PiecewiseConstantWarmUp\"):\n",
    "            step = tf.cast(tf.convert_to_tensor(step), tf.float32)\n",
    "            pred_fn_pairs = []\n",
    "            warmup_steps = self.warmup_steps\n",
    "            boundaries = self.boundaries\n",
    "            values = self.values\n",
    "            min_lr = self.min_lr\n",
    "\n",
    "            pred_fn_pairs.append(\n",
    "                (step <= warmup_steps,\n",
    "                 lambda: min_lr + step * (values[0] - min_lr) / warmup_steps))\n",
    "            pred_fn_pairs.append(\n",
    "                (tf.logical_and(step <= boundaries[0],\n",
    "                                step > warmup_steps),\n",
    "                 lambda: tf.constant(values[0])))\n",
    "            pred_fn_pairs.append(\n",
    "                (step > boundaries[-1], lambda: tf.constant(values[-1])))\n",
    "\n",
    "            for low, high, v in zip(boundaries[:-1], boundaries[1:],\n",
    "                                    values[1:-1]):\n",
    "                # Need to bind v here; can do this with lambda v=v: ...\n",
    "                pred = (step > low) & (step <= high)\n",
    "                pred_fn_pairs.append((pred, lambda: tf.constant(v)))\n",
    "\n",
    "            # The default isn't needed here because our conditions are mutually\n",
    "            # exclusive and exhaustive, but tf.case requires it.\n",
    "            return tf.case(pred_fn_pairs, lambda: tf.constant(values[0]),\n",
    "                           exclusive=True)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "                \"boundaries\": self.boundaries,\n",
    "                \"values\": self.values,\n",
    "                \"warmup_steps\": self.warmup_steps,\n",
    "                \"min_lr\": self.min_lr,\n",
    "                \"name\": self.name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiStepWarmUpLR(initial_learning_rate, lr_steps, lr_rate,\n",
    "                      warmup_steps=0., min_lr=0.,\n",
    "                      name='MultiStepWarmUpLR'):\n",
    "    \"\"\"Multi-steps warm up learning rate scheduler.\"\"\"\n",
    "    assert warmup_steps <= lr_steps[0]\n",
    "    assert min_lr <= initial_learning_rate\n",
    "    lr_steps_value = [initial_learning_rate]\n",
    "    for _ in range(len(lr_steps)):\n",
    "        lr_steps_value.append(lr_steps_value[-1] * lr_rate)\n",
    "    return PiecewiseConstantWarmUpDecay(\n",
    "        boundaries=lr_steps, values=lr_steps_value, warmup_steps=warmup_steps,\n",
    "        min_lr=min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard negative mining\n",
    "- Object Detection 모델 학습시 자주 사용되는 Hard negative mining이라는 기법이 있습니다. 학습과정에서 label은 negative인데 confidence가 높게 나오는 샘플을 재학습하면 positive와 negative의 모호한 경계선상에 분포한 false negative 오류에 강해진다는 장점이 있습니다.\n",
    "\n",
    "- 실제로 confidence가 높은 샘플을 모아 training을 다시 수행하기보다는, 그런 샘플들에 대한 loss만 따로 모아 계산해주는 방식으로 반영할 수 있습니다.\n",
    "\n",
    "- 아래 구현된 hard_negative_mining 메소드와, 이 메소드를 통해 얻은 샘플을 통해 얻은 localization loss를 기존의 classification loss에 추가로 반영하는 MultiBoxLoss 계산 메소드를 확인해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_negative_mining(loss, class_truth, neg_ratio):\n",
    "    \"\"\" Hard negative mining algorithm\n",
    "        to pick up negative examples for back-propagation\n",
    "        base on classification loss values\n",
    "    Args:\n",
    "        loss: list of classification losses of all default boxes (B, num_default)\n",
    "        class_truth: classification targets (B, num_default)\n",
    "        neg_ratio: negative / positive ratio\n",
    "    Returns:\n",
    "        class_loss: classification loss\n",
    "        loc_loss: regression loss\n",
    "    \"\"\"\n",
    "    # loss: B x N\n",
    "    # class_truth: B x N\n",
    "    pos_idx = class_truth > 0\n",
    "    num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
    "    num_neg = num_pos * neg_ratio\n",
    "\n",
    "    rank = tf.argsort(loss, axis=1, direction='DESCENDING')\n",
    "    rank = tf.argsort(rank, axis=1)\n",
    "    neg_idx = rank < tf.expand_dims(num_neg, 1)\n",
    "\n",
    "    return pos_idx, neg_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiBoxLoss(num_class=3, neg_pos_ratio=3.0):\n",
    "    def multi_loss(y_true, y_pred):\n",
    "        \"\"\" Compute losses for SSD\n",
    "               regression loss: smooth L1\n",
    "               classification loss: cross entropy\n",
    "           Args:\n",
    "               y_true: [B,N,5]\n",
    "               y_pred: [B,N,num_class]\n",
    "               class_pred: outputs of classification heads (B,N, num_classes)\n",
    "               loc_pred: outputs of regression heads (B,N, 4)\n",
    "               class_truth: classification targets (B,N)\n",
    "               loc_truth: regression targets (B,N, 4)\n",
    "           Returns:\n",
    "               class_loss: classification loss\n",
    "               loc_loss: regression loss\n",
    "       \"\"\"\n",
    "        num_batch = tf.shape(y_true)[0]\n",
    "        num_prior = tf.shape(y_true)[1]\n",
    "        loc_pred, class_pred = y_pred[..., :4], y_pred[..., 4:]\n",
    "        loc_truth, class_truth = y_true[..., :4], tf.squeeze(y_true[..., 4:])\n",
    "\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "        # compute classification losses without reduction\n",
    "        temp_loss = cross_entropy(class_truth, class_pred)\n",
    "        # 2. hard negative mining\n",
    "        pos_idx, neg_idx = hard_negative_mining(temp_loss, class_truth, neg_pos_ratio)\n",
    "\n",
    "        # classification loss will consist of positive and negative examples\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='sum')\n",
    "\n",
    "        smooth_l1_loss = tf.keras.losses.Huber(reduction='sum')\n",
    "\n",
    "        loss_class = cross_entropy(\n",
    "            class_truth[tf.math.logical_or(pos_idx, neg_idx)],\n",
    "            class_pred[tf.math.logical_or(pos_idx, neg_idx)])\n",
    "\n",
    "        # localization loss only consist of positive examples (smooth L1)\n",
    "        loss_loc = smooth_l1_loss(loc_truth[pos_idx],loc_pred[pos_idx])\n",
    "\n",
    "        num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.float32))\n",
    "\n",
    "        loss_class = loss_class / num_pos\n",
    "        loss_loc = loss_loc / num_pos\n",
    "        return loss_loc, loss_class\n",
    "\n",
    "    return multi_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "- 이제 본격적으로 모델 학습을 진행하겠습니다. 배치사이즈, epoch 수 등 학습에 대한 기본설정은 cfg dict 내용을 확인해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "global load_t1\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logger = tf.get_logger()\n",
    "logger.disabled = True\n",
    "logger.setLevel(logging.FATAL)\n",
    "\n",
    "weights_dir = os.getenv('HOME')+'/aiffel/face_detector/checkpoints'\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.mkdir(weights_dir)\n",
    "\n",
    "logging.info(\"Load configuration...\")\n",
    "label_classes = cfg['labels_list']\n",
    "logging.info(f\"Total image sample:{cfg['dataset_len']},Total classes number:\"\n",
    "             f\"{len(label_classes)},classes list:{label_classes}\")\n",
    "\n",
    "logging.info(\"Compute prior boxes...\")\n",
    "priors, num_cell = prior_box(cfg)\n",
    "logging.info(f\"Prior boxes number:{len(priors)},default anchor box number per feature map cell:{num_cell}\") # 4420, [3, 2, 2, 3]\n",
    "\n",
    "logging.info(\"Loading dataset...\")\n",
    "train_dataset = load_dataset(cfg, priors, shuffle=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ssd_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, 240, 320, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_43 (ZeroPadding2D)     (None, 242, 322, 3)  0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_43 (Conv2D)                (None, 120, 160, 16) 432         conv_pad_43[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_43 (BatchNormalization) (None, 120, 160, 16) 64          conv_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_43 (ReLU)             (None, 120, 160, 16) 0           conv_bn_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_44 (Conv2D)                (None, 120, 160, 32) 4608        conv_relu_43[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_44 (BatchNormalization) (None, 120, 160, 32) 128         conv_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_44 (ReLU)             (None, 120, 160, 32) 0           conv_bn_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_45 (ZeroPadding2D)     (None, 122, 162, 32) 0           conv_relu_44[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_45 (Conv2D)                (None, 60, 80, 32)   9216        conv_pad_45[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_45 (BatchNormalization) (None, 60, 80, 32)   128         conv_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_45 (ReLU)             (None, 60, 80, 32)   0           conv_bn_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_46 (Conv2D)                (None, 60, 80, 32)   9216        conv_relu_45[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_46 (BatchNormalization) (None, 60, 80, 32)   128         conv_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_46 (ReLU)             (None, 60, 80, 32)   0           conv_bn_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_47 (ZeroPadding2D)     (None, 62, 82, 32)   0           conv_relu_46[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_47 (Conv2D)                (None, 30, 40, 64)   18432       conv_pad_47[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_47 (BatchNormalization) (None, 30, 40, 64)   256         conv_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_47 (ReLU)             (None, 30, 40, 64)   0           conv_bn_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_48 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_47[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_48 (BatchNormalization) (None, 30, 40, 64)   256         conv_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_48 (ReLU)             (None, 30, 40, 64)   0           conv_bn_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_49 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_48[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_49 (BatchNormalization) (None, 30, 40, 64)   256         conv_49[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_49 (ReLU)             (None, 30, 40, 64)   0           conv_bn_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_50 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_49[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_50 (BatchNormalization) (None, 30, 40, 64)   256         conv_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_50 (ReLU)             (None, 30, 40, 64)   0           conv_bn_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_51 (ZeroPadding2D)     (None, 32, 42, 64)   0           conv_relu_50[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_51 (Conv2D)                (None, 15, 20, 128)  73728       conv_pad_51[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_51 (BatchNormalization) (None, 15, 20, 128)  512         conv_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_51 (ReLU)             (None, 15, 20, 128)  0           conv_bn_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_52 (Conv2D)                (None, 15, 20, 128)  147456      conv_relu_51[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_52 (BatchNormalization) (None, 15, 20, 128)  512         conv_52[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_52 (ReLU)             (None, 15, 20, 128)  0           conv_bn_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_53 (Conv2D)                (None, 15, 20, 128)  147456      conv_relu_52[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_53 (BatchNormalization) (None, 15, 20, 128)  512         conv_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_53 (ReLU)             (None, 15, 20, 128)  0           conv_bn_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_54 (ZeroPadding2D)     (None, 17, 22, 128)  0           conv_relu_53[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_54 (DepthwiseConv2D)    (None, 8, 10, 128)   1152        conv_pad_54[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_54_bn (BatchNormalizati (None, 8, 10, 128)   512         conv_dw_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_54_relu (ReLU)          (None, 8, 10, 128)   0           conv_dw_54_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_54 (Conv2D)             (None, 8, 10, 256)   32768       conv_dw_54_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_54_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_pw_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_54_relu (ReLU)          (None, 8, 10, 256)   0           conv_pw_54_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_55 (DepthwiseConv2D)    (None, 8, 10, 256)   2304        conv_pw_54_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_55_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_dw_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_55_relu (ReLU)          (None, 8, 10, 256)   0           conv_dw_55_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_55 (Conv2D)             (None, 8, 10, 256)   65536       conv_dw_55_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_55_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_pw_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_55_relu (ReLU)          (None, 8, 10, 256)   0           conv_pw_55_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_56 (ZeroPadding2D)     (None, 10, 12, 256)  0           conv_pw_55_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_56 (DepthwiseConv2D)    (None, 4, 5, 256)    2304        conv_pad_56[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_56_bn (BatchNormalizati (None, 4, 5, 256)    1024        conv_dw_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_56_relu (ReLU)          (None, 4, 5, 256)    0           conv_dw_56_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_56 (Conv2D)             (None, 4, 5, 256)    65536       conv_dw_56_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_56_bn (BatchNormalizati (None, 4, 5, 256)    1024        conv_pw_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_56_relu (ReLU)          (None, 4, 5, 256)    0           conv_pw_56_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 30, 40, 16)   9232        conv_relu_50[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 15, 20, 16)   18448       conv_relu_53[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 8, 10, 16)    36880       conv_pw_55_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 4, 5, 16)     36880       conv_pw_56_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 30, 40, 16)   0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 15, 20, 16)   0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 8, 10, 16)    0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 4, 5, 16)     0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 30, 40, 16)   2320        leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 30, 40, 32)   18464       conv_relu_50[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 15, 20, 16)   2320        leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 15, 20, 32)   36896       conv_relu_53[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 8, 10, 16)    2320        leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 8, 10, 32)    73760       conv_pw_55_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 4, 5, 16)     2320        leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 4, 5, 32)     73760       conv_pw_56_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 30, 40, 48)   0           conv2d_61[0][0]                  \n",
      "                                                                 conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 15, 20, 48)   0           conv2d_64[0][0]                  \n",
      "                                                                 conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 8, 10, 48)    0           conv2d_67[0][0]                  \n",
      "                                                                 conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 4, 5, 48)     0           conv2d_70[0][0]                  \n",
      "                                                                 conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 30, 40, 48)   0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 15, 20, 48)   0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 8, 10, 48)    0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 4, 5, 48)     0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 30, 40, 12)   5196        re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 15, 20, 8)    3464        re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 10, 8)     3464        re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 4, 5, 12)     5196        re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 30, 40, 6)    2598        re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 15, 20, 4)    1732        re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 10, 4)     1732        re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 4, 5, 6)      2598        re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_25 (Reshape)            (None, 3600, 4)      0           conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_27 (Reshape)            (None, 600, 4)       0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_29 (Reshape)            (None, 160, 4)       0           conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_31 (Reshape)            (None, 60, 4)        0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_24 (Reshape)            (None, 3600, 2)      0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_26 (Reshape)            (None, 600, 2)       0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_28 (Reshape)            (None, 160, 2)       0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_30 (Reshape)            (None, 60, 2)        0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_boxes (Concatenate)        (None, 4420, 4)      0           reshape_25[0][0]                 \n",
      "                                                                 reshape_27[0][0]                 \n",
      "                                                                 reshape_29[0][0]                 \n",
      "                                                                 reshape_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "face_classes (Concatenate)      (None, 4420, 2)      0           reshape_24[0][0]                 \n",
      "                                                                 reshape_26[0][0]                 \n",
      "                                                                 reshape_28[0][0]                 \n",
      "                                                                 reshape_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, 4420, 6)      0           face_boxes[0][0]                 \n",
      "                                                                 face_classes[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,038,956\n",
      "Trainable params: 1,034,636\n",
      "Non-trainable params: 4,320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Create Model...\")\n",
    "try:\n",
    "    model = SsdModel(cfg=cfg, num_cell=num_cell, training=True)\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, to_file=os.path.join(os.getcwd(), 'model.png'),\n",
    "                              show_shapes=True, show_layer_names=True)\n",
    "except Exception as e:\n",
    "    logging.error(e)\n",
    "    logging.info(\"Create network failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['resume']:\n",
    "    # Training from latest weights\n",
    "    paths = [os.path.join(weights_dir, path)\n",
    "             for path in os.listdir(weights_dir)]\n",
    "    latest = sorted(paths, key=os.path.getmtime)[-1]\n",
    "    model.load_weights(latest)\n",
    "    init_epoch = int(os.path.splitext(latest)[0][-3:])\n",
    "\n",
    "else:\n",
    "    init_epoch = -1\n",
    "\n",
    "steps_per_epoch = cfg['dataset_len'] // cfg['batch_size']\n",
    "logging.info(f\"steps_per_epoch:{steps_per_epoch}\")\n",
    "\n",
    "learning_rate = MultiStepWarmUpLR(\n",
    "    initial_learning_rate=cfg['init_lr'],\n",
    "    lr_steps=[e * steps_per_epoch for e in cfg['lr_decay_epoch']],\n",
    "    lr_rate=cfg['lr_rate'],\n",
    "    warmup_steps=cfg['warmup_epoch'] * steps_per_epoch,\n",
    "    min_lr=cfg['min_lr'])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=cfg['momentum'], nesterov=True)\n",
    "multi_loss = MultiBoxLoss(num_class=len(label_classes), neg_pos_ratio=3)\n",
    "train_log_dir = 'logs/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        losses = {}\n",
    "        losses['reg'] = tf.reduce_sum(model.losses)  #unused. Init for redefine network\n",
    "        losses['loc'], losses['class'] = multi_loss(labels, predictions)\n",
    "        total_loss = tf.add_n([l for l in losses.values()])\n",
    "\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return total_loss, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래에서 본격적으로 train을 시작합니다. 1Epoch당 1분 가량 소요되며, 총 200Epoch 학습이 진행됩니다. 소요시간에 유의해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200 | Batch 402/402 | Batch time 0.062 || Loss: 10.075412 | loc loss:6.480650 | class loss:3.594762  \n",
      "Epoch: 1/200  | Epoch time 91.993 || Average Loss: inf\n",
      "Epoch: 2/200 | Batch 402/402 | Batch time 0.103 || Loss: 5.801622 | loc loss:3.866320 | class loss:1.935302  \n",
      "Epoch: 2/200  | Epoch time 50.773 || Average Loss: inf\n",
      "Epoch: 3/200 | Batch 402/402 | Batch time 0.106 || Loss: 5.785058 | loc loss:3.757749 | class loss:2.027309  \n",
      "Epoch: 3/200  | Epoch time 50.220 || Average Loss: inf\n",
      "Epoch: 4/200 | Batch 402/402 | Batch time 0.100 || Loss: 6.713247 | loc loss:4.376880 | class loss:2.336367 \n",
      "Epoch: 4/200  | Epoch time 50.533 || Average Loss: inf\n",
      "Epoch: 5/200 | Batch 402/402 | Batch time 0.133 || Loss: 5.675443 | loc loss:3.953106 | class loss:1.722338  \n",
      "Epoch: 5/200  | Epoch time 48.700 || Average Loss: inf\n",
      "Epoch: 6/200 | Batch 402/402 | Batch time 0.118 || Loss: 5.680511 | loc loss:3.990409 | class loss:1.690102 \n",
      "Epoch: 6/200  | Epoch time 48.597 || Average Loss: inf\n",
      "Epoch: 7/200 | Batch 402/402 | Batch time 0.124 || Loss: 6.948552 | loc loss:5.098095 | class loss:1.850456  \n",
      "Epoch: 7/200  | Epoch time 49.555 || Average Loss: inf\n",
      "Epoch: 8/200 | Batch 402/402 | Batch time 0.103 || Loss: 4.896423 | loc loss:3.290179 | class loss:1.606245 \n",
      "Epoch: 8/200  | Epoch time 51.096 || Average Loss: inf\n",
      "Epoch: 9/200 | Batch 402/402 | Batch time 0.100 || Loss: 2.866385 | loc loss:1.547962 | class loss:1.318423 \n",
      "Epoch: 9/200  | Epoch time 50.983 || Average Loss: inf\n",
      "Epoch: 10/200 | Batch 402/402 | Batch time 0.110 || Loss: 4.072371 | loc loss:2.326439 | class loss:1.745933 \n",
      "Epoch: 10/200  | Epoch time 50.828 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel0042/aiffel/face_detector/checkpoints/weights_epoch_010.h5<<<<<<<<<<\n",
      "Epoch: 11/200 | Batch 402/402 | Batch time 0.122 || Loss: 4.809221 | loc loss:3.096530 | class loss:1.712690 \n",
      "Epoch: 11/200  | Epoch time 48.930 || Average Loss: inf\n",
      "Epoch: 12/200 | Batch 402/402 | Batch time 0.135 || Loss: 4.649401 | loc loss:2.964530 | class loss:1.684871 \n",
      "Epoch: 12/200  | Epoch time 50.150 || Average Loss: inf\n",
      "Epoch: 13/200 | Batch 402/402 | Batch time 0.149 || Loss: 5.125986 | loc loss:3.715474 | class loss:1.410512 \n",
      "Epoch: 13/200  | Epoch time 50.025 || Average Loss: inf\n",
      "Epoch: 14/200 | Batch 402/402 | Batch time 0.112 || Loss: 4.288555 | loc loss:2.793767 | class loss:1.494788 \n",
      "Epoch: 14/200  | Epoch time 49.359 || Average Loss: inf\n",
      "Epoch: 15/200 | Batch 402/402 | Batch time 0.125 || Loss: 5.526395 | loc loss:4.098047 | class loss:1.428348 \n",
      "Epoch: 15/200  | Epoch time 49.474 || Average Loss: inf\n",
      "Epoch: 16/200 | Batch 402/402 | Batch time 0.113 || Loss: 5.293997 | loc loss:3.906667 | class loss:1.387330 \n",
      "Epoch: 16/200  | Epoch time 49.461 || Average Loss: inf\n",
      "Epoch: 17/200 | Batch 402/402 | Batch time 0.128 || Loss: 5.559551 | loc loss:3.968665 | class loss:1.590886  \n",
      "Epoch: 17/200  | Epoch time 49.508 || Average Loss: inf\n",
      "Epoch: 18/200 | Batch 402/402 | Batch time 0.132 || Loss: 6.080979 | loc loss:4.542402 | class loss:1.538577 \n",
      "Epoch: 18/200  | Epoch time 48.602 || Average Loss: inf\n",
      "Epoch: 19/200 | Batch 402/402 | Batch time 0.144 || Loss: 4.293734 | loc loss:2.782656 | class loss:1.511077 \n",
      "Epoch: 19/200  | Epoch time 49.203 || Average Loss: inf\n",
      "Epoch: 20/200 | Batch 402/402 | Batch time 0.137 || Loss: 6.430413 | loc loss:5.133317 | class loss:1.297096  \n",
      "Epoch: 20/200  | Epoch time 49.800 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel0042/aiffel/face_detector/checkpoints/weights_epoch_020.h5<<<<<<<<<<\n",
      "Epoch: 21/200 | Batch 402/402 | Batch time 0.099 || Loss: 4.876135 | loc loss:3.222732 | class loss:1.653404 \n",
      "Epoch: 21/200  | Epoch time 49.401 || Average Loss: inf\n",
      "Epoch: 22/200 | Batch 402/402 | Batch time 0.119 || Loss: 5.446833 | loc loss:4.359642 | class loss:1.087191 \n",
      "Epoch: 22/200  | Epoch time 49.485 || Average Loss: inf\n",
      "Epoch: 23/200 | Batch 402/402 | Batch time 0.100 || Loss: 4.653727 | loc loss:3.533198 | class loss:1.120528 \n",
      "Epoch: 23/200  | Epoch time 50.045 || Average Loss: inf\n",
      "Epoch: 24/200 | Batch 402/402 | Batch time 0.110 || Loss: 5.012391 | loc loss:3.697161 | class loss:1.315229 \n",
      "Epoch: 24/200  | Epoch time 49.313 || Average Loss: inf\n",
      "Epoch: 25/200 | Batch 402/402 | Batch time 0.114 || Loss: 5.324081 | loc loss:3.883008 | class loss:1.441074 \n",
      "Epoch: 25/200  | Epoch time 50.368 || Average Loss: inf\n",
      "Epoch: 26/200 | Batch 402/402 | Batch time 0.127 || Loss: 4.269423 | loc loss:2.881731 | class loss:1.387692 \n",
      "Epoch: 26/200  | Epoch time 49.718 || Average Loss: inf\n",
      "Epoch: 27/200 | Batch 402/402 | Batch time 0.147 || Loss: 4.374786 | loc loss:2.889850 | class loss:1.484936 \n",
      "Epoch: 27/200  | Epoch time 50.358 || Average Loss: inf\n",
      "Epoch: 28/200 | Batch 402/402 | Batch time 0.119 || Loss: 2.748699 | loc loss:1.632496 | class loss:1.116203 \n",
      "Epoch: 28/200  | Epoch time 49.347 || Average Loss: inf\n",
      "Epoch: 29/200 | Batch 402/402 | Batch time 0.131 || Loss: 5.013031 | loc loss:3.607807 | class loss:1.405223 \n",
      "Epoch: 29/200  | Epoch time 49.955 || Average Loss: inf\n",
      "Epoch: 30/200 | Batch 402/402 | Batch time 0.121 || Loss: 4.611797 | loc loss:3.411550 | class loss:1.200247 \n",
      "Epoch: 30/200  | Epoch time 50.121 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel0042/aiffel/face_detector/checkpoints/weights_epoch_030.h5<<<<<<<<<<\n",
      "Epoch: 31/200 | Batch 402/402 | Batch time 0.115 || Loss: 4.534105 | loc loss:3.448228 | class loss:1.085876 \n",
      "Epoch: 31/200  | Epoch time 49.766 || Average Loss: inf\n",
      "Epoch: 32/200 | Batch 402/402 | Batch time 0.124 || Loss: 5.375616 | loc loss:3.951207 | class loss:1.424409 \n",
      "Epoch: 32/200  | Epoch time 50.553 || Average Loss: inf\n",
      "Epoch: 33/200 | Batch 402/402 | Batch time 0.130 || Loss: 5.248471 | loc loss:3.949290 | class loss:1.299181 \n",
      "Epoch: 33/200  | Epoch time 50.030 || Average Loss: inf\n",
      "Epoch: 34/200 | Batch 402/402 | Batch time 0.117 || Loss: 5.064035 | loc loss:3.737790 | class loss:1.326245 \n",
      "Epoch: 34/200  | Epoch time 50.528 || Average Loss: inf\n",
      "Epoch: 35/200 | Batch 402/402 | Batch time 0.113 || Loss: 4.440466 | loc loss:3.301204 | class loss:1.139262 \n",
      "Epoch: 35/200  | Epoch time 50.151 || Average Loss: inf\n",
      "Epoch: 36/200 | Batch 402/402 | Batch time 0.143 || Loss: 6.405334 | loc loss:5.170563 | class loss:1.234771 \n",
      "Epoch: 36/200  | Epoch time 50.394 || Average Loss: inf\n",
      "Epoch: 37/200 | Batch 402/402 | Batch time 0.103 || Loss: 6.347378 | loc loss:4.715050 | class loss:1.632328 \n",
      "Epoch: 37/200  | Epoch time 49.080 || Average Loss: inf\n",
      "Epoch: 38/200 | Batch 402/402 | Batch time 0.107 || Loss: 7.854342 | loc loss:6.669972 | class loss:1.184371 \n",
      "Epoch: 38/200  | Epoch time 50.622 || Average Loss: inf\n",
      "Epoch: 39/200 | Batch 402/402 | Batch time 0.116 || Loss: 4.562076 | loc loss:2.951974 | class loss:1.610101 \n",
      "Epoch: 39/200  | Epoch time 49.817 || Average Loss: inf\n",
      "Epoch: 40/200 | Batch 402/402 | Batch time 0.117 || Loss: 5.554204 | loc loss:4.506925 | class loss:1.047279 \n",
      "Epoch: 40/200  | Epoch time 50.086 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel0042/aiffel/face_detector/checkpoints/weights_epoch_040.h5<<<<<<<<<<\n",
      "Epoch: 41/200 | Batch 402/402 | Batch time 0.105 || Loss: 3.948282 | loc loss:2.766991 | class loss:1.181291 \n",
      "Epoch: 41/200  | Epoch time 50.047 || Average Loss: inf\n",
      "Epoch: 42/200 | Batch 402/402 | Batch time 0.096 || Loss: 5.058130 | loc loss:3.933266 | class loss:1.124864 \n",
      "Epoch: 42/200  | Epoch time 49.552 || Average Loss: inf\n",
      "Epoch: 43/200 | Batch 402/402 | Batch time 0.121 || Loss: 4.645550 | loc loss:3.260344 | class loss:1.385205 \n",
      "Epoch: 43/200  | Epoch time 50.424 || Average Loss: inf\n",
      "Epoch: 44/200 | Batch 402/402 | Batch time 0.117 || Loss: 5.851381 | loc loss:4.603804 | class loss:1.247577 \n",
      "Epoch: 44/200  | Epoch time 49.590 || Average Loss: inf\n",
      "Epoch: 45/200 | Batch 402/402 | Batch time 0.104 || Loss: 4.259782 | loc loss:3.230587 | class loss:1.029195 \n",
      "Epoch: 45/200  | Epoch time 46.348 || Average Loss: inf\n",
      "Epoch: 46/200 | Batch 402/402 | Batch time 0.087 || Loss: 4.526150 | loc loss:3.221044 | class loss:1.305106 \n",
      "Epoch: 46/200  | Epoch time 45.455 || Average Loss: inf\n",
      "Epoch: 47/200 | Batch 402/402 | Batch time 0.098 || Loss: 4.383545 | loc loss:3.487175 | class loss:0.896371 \n",
      "Epoch: 47/200  | Epoch time 45.885 || Average Loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48/200 | Batch 402/402 | Batch time 0.112 || Loss: 3.686270 | loc loss:2.588318 | class loss:1.097952 \n",
      "Epoch: 48/200  | Epoch time 45.370 || Average Loss: inf\n",
      "Epoch: 49/200 | Batch 402/402 | Batch time 0.116 || Loss: 4.481869 | loc loss:3.363650 | class loss:1.118219 \n",
      "Epoch: 49/200  | Epoch time 45.539 || Average Loss: inf\n",
      "Epoch: 50/200 | Batch 402/402 | Batch time 0.096 || Loss: 3.361802 | loc loss:2.343446 | class loss:1.018356 \n",
      "Epoch: 50/200  | Epoch time 45.907 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel0042/aiffel/face_detector/checkpoints/weights_epoch_050.h5<<<<<<<<<<\n",
      "Epoch: 51/200 | Batch 402/402 | Batch time 0.111 || Loss: 6.399289 | loc loss:4.802516 | class loss:1.596773 \n",
      "Epoch: 51/200  | Epoch time 45.345 || Average Loss: inf\n",
      "Epoch: 52/200 | Batch 402/402 | Batch time 0.114 || Loss: 3.700517 | loc loss:2.778605 | class loss:0.921913 \n",
      "Epoch: 52/200  | Epoch time 45.411 || Average Loss: inf\n",
      "Epoch: 53/200 | Batch 402/402 | Batch time 0.108 || Loss: 2.651483 | loc loss:1.853208 | class loss:0.798274 \n",
      "Epoch: 53/200  | Epoch time 45.524 || Average Loss: inf\n",
      "Epoch: 54/200 | Batch 402/402 | Batch time 0.131 || Loss: 4.920662 | loc loss:3.750576 | class loss:1.170086 \n",
      "Epoch: 54/200  | Epoch time 45.832 || Average Loss: inf\n",
      "Epoch: 55/200 | Batch 402/402 | Batch time 0.100 || Loss: 3.121432 | loc loss:2.046771 | class loss:1.074662 \n",
      "Epoch: 55/200  | Epoch time 45.097 || Average Loss: inf\n",
      "Epoch: 56/200 | Batch 402/402 | Batch time 0.159 || Loss: 4.281869 | loc loss:2.992800 | class loss:1.289069 \n",
      "Epoch: 56/200  | Epoch time 45.256 || Average Loss: inf\n",
      "Epoch: 57/200 | Batch 402/402 | Batch time 0.104 || Loss: 4.447765 | loc loss:3.216159 | class loss:1.231606 \n",
      "Epoch: 57/200  | Epoch time 45.162 || Average Loss: inf\n",
      "Epoch: 58/200 | Batch 402/402 | Batch time 0.105 || Loss: 3.887314 | loc loss:2.891563 | class loss:0.995751 \n",
      "Epoch: 58/200  | Epoch time 45.662 || Average Loss: inf\n",
      "Epoch: 59/200 | Batch 402/402 | Batch time 0.102 || Loss: 3.623590 | loc loss:2.440675 | class loss:1.182914 \n",
      "Epoch: 59/200  | Epoch time 45.886 || Average Loss: inf\n",
      "Epoch: 60/200 | Batch 402/402 | Batch time 0.104 || Loss: 4.060990 | loc loss:3.069490 | class loss:0.991500 \n",
      "Epoch: 60/200  | Epoch time 46.037 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel0042/aiffel/face_detector/checkpoints/weights_epoch_060.h5<<<<<<<<<<\n",
      "Epoch: 61/200 | Batch 402/402 | Batch time 0.092 || Loss: 5.845579 | loc loss:4.647328 | class loss:1.198251 \n",
      "Epoch: 61/200  | Epoch time 45.792 || Average Loss: inf\n",
      "Epoch: 62/200 | Batch 402/402 | Batch time 0.111 || Loss: 4.354712 | loc loss:3.480197 | class loss:0.874515 \n",
      "Epoch: 62/200  | Epoch time 45.549 || Average Loss: inf\n",
      "Epoch: 63/200 | Batch 402/402 | Batch time 0.112 || Loss: 5.269700 | loc loss:4.207613 | class loss:1.062087  \n",
      "Epoch: 63/200  | Epoch time 44.869 || Average Loss: inf\n",
      "Epoch: 64/200 | Batch 402/402 | Batch time 0.125 || Loss: 3.021907 | loc loss:2.203691 | class loss:0.818216 \n",
      "Epoch: 64/200  | Epoch time 45.120 || Average Loss: inf\n",
      "Epoch: 65/200 | Batch 402/402 | Batch time 0.109 || Loss: 3.310453 | loc loss:2.213692 | class loss:1.096761 \n",
      "Epoch: 65/200  | Epoch time 45.739 || Average Loss: inf\n",
      "Epoch: 66/200 | Batch 402/402 | Batch time 0.107 || Loss: 4.197197 | loc loss:3.099533 | class loss:1.097664 \n",
      "Epoch: 66/200  | Epoch time 46.193 || Average Loss: inf\n",
      "Epoch: 67/200 | Batch 402/402 | Batch time 0.115 || Loss: 5.711740 | loc loss:4.482756 | class loss:1.228983 \n",
      "Epoch: 67/200  | Epoch time 46.903 || Average Loss: inf\n",
      "Epoch: 68/200 | Batch 402/402 | Batch time 0.098 || Loss: 4.952598 | loc loss:3.972481 | class loss:0.980116 \n",
      "Epoch: 68/200  | Epoch time 45.876 || Average Loss: inf\n",
      "Epoch: 69/200 | Batch 402/402 | Batch time 0.120 || Loss: 3.914170 | loc loss:2.955036 | class loss:0.959134 \n",
      "Epoch: 69/200  | Epoch time 49.741 || Average Loss: inf\n",
      "Epoch: 70/200 | Batch 402/402 | Batch time 0.113 || Loss: 3.861087 | loc loss:2.929025 | class loss:0.932062 \n",
      "Epoch: 70/200  | Epoch time 51.995 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel0042/aiffel/face_detector/checkpoints/weights_epoch_070.h5<<<<<<<<<<\n",
      "Epoch: 71/200 | Batch 402/402 | Batch time 0.131 || Loss: 3.582206 | loc loss:2.461461 | class loss:1.120745 \n",
      "Epoch: 71/200  | Epoch time 50.895 || Average Loss: inf\n",
      "Epoch: 72/200 | Batch 402/402 | Batch time 0.122 || Loss: 5.990700 | loc loss:5.119151 | class loss:0.871549 \n",
      "Epoch: 72/200  | Epoch time 50.112 || Average Loss: inf\n",
      "Epoch: 73/200 | Batch 402/402 | Batch time 0.106 || Loss: 3.296921 | loc loss:2.212319 | class loss:1.084603 \n",
      "Epoch: 73/200  | Epoch time 50.717 || Average Loss: inf\n",
      "Epoch: 74/200 | Batch 186/402 | Batch time 0.115 || Loss: 4.304731 | loc loss:3.397553 | class loss:0.907178 "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(init_epoch+1,cfg['epoch']):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        avg_loss = 0.0\n",
    "        for step, (inputs, labels) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "\n",
    "            load_t0 = time.time()\n",
    "            total_loss, losses = train_step(inputs, labels)\n",
    "            avg_loss = (avg_loss * step + total_loss.numpy()) / (step + 1)\n",
    "            load_t1 = time.time()\n",
    "            batch_time = load_t1 - load_t0\n",
    "\n",
    "            steps =steps_per_epoch*epoch+step\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss/total_loss', total_loss, step=steps)\n",
    "                for k, l in losses.items():\n",
    "                    tf.summary.scalar('loss/{}'.format(k), l, step=steps)\n",
    "                tf.summary.scalar('learning_rate', optimizer.lr(steps), step=steps)\n",
    "\n",
    "            print(f\"\\rEpoch: {epoch + 1}/{cfg['epoch']} | Batch {step + 1}/{steps_per_epoch} | Batch time {batch_time:.3f} || Loss: {total_loss:.6f} | loc loss:{losses['loc']:.6f} | class loss:{losses['class']:.6f} \",end = '',flush=True)\n",
    "\n",
    "        print(f\"\\nEpoch: {epoch + 1}/{cfg['epoch']}  | Epoch time {(load_t1 - start):.3f} || Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss/avg_loss',avg_loss,step=epoch)\n",
    "\n",
    "        if (epoch + 1) % cfg['save_freq'] == 0:\n",
    "            filepath = os.path.join(weights_dir, f'weights_epoch_{(epoch + 1):03d}.h5')\n",
    "            model.save_weights(filepath)\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\">>>>>>>>>>Save weights file at {filepath}<<<<<<<<<<\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted')\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 스텝에 소개한 모델 학습 과정을 train.py에 정리해 두었습니다. 아래와 같이 위의 과정을 실행할 수 있습니다.\n",
    "\n",
    "- $ cd ~/aiffel/face_detector && python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference(1) NMS\n",
    "## NMS 구현하기\n",
    "- Grid cell을 사용하는 Object detection의 inference 단계에서 하나의 object가 여러 개의 prior box에 걸쳐져 있을 때 가장 확률이 높은 1개의 prior box를 하나로 줄여주는 NMS(non-max suppression)이 필요합니다. 아래 코드를 확인해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_bbox_tf(pre, priors, variances=None):\n",
    "    \"\"\"Decode locations from predictions using prior to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        pre (tensor): location predictions for loc layers,\n",
    "            Shape: [num_prior,4]\n",
    "        prior (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_prior,4].\n",
    "        variances: (list[float]) Variances of prior boxes\n",
    "    Return:\n",
    "        decoded bounding box predictions xmin, ymin, xmax, ymax\n",
    "    \"\"\"\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "    centers = priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:]\n",
    "    sides = priors[:, 2:] * tf.math.exp(pre[:, 2:] * variances[1])\n",
    "\n",
    "    return tf.concat([centers - sides / 2, centers + sides / 2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nms(boxes, scores, nms_threshold=0.5, limit=200):\n",
    "    \"\"\" Perform Non Maximum Suppression algorithm\n",
    "        to eliminate boxes with high overlap\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "        scores: tensor (num_boxes,)\n",
    "        nms_threshold: NMS threshold\n",
    "        limit: maximum number of boxes to keep\n",
    "    Returns:\n",
    "        idx: indices of kept boxes\n",
    "    \"\"\"\n",
    "    if boxes.shape[0] == 0:\n",
    "        return tf.constant([], dtype=tf.int32)\n",
    "    selected = [0]\n",
    "    idx = tf.argsort(scores, direction='DESCENDING')\n",
    "    idx = idx[:limit]\n",
    "    boxes = tf.gather(boxes, idx)\n",
    "\n",
    "    iou = _jaccard(boxes, boxes)\n",
    "\n",
    "    while True:\n",
    "        row = iou[selected[-1]]\n",
    "        next_indices = row <= nms_threshold\n",
    "\n",
    "        iou = tf.where(\n",
    "            tf.expand_dims(tf.math.logical_not(next_indices), 0),\n",
    "            tf.ones_like(iou, dtype=tf.float32),\n",
    "            iou)\n",
    "\n",
    "        if not tf.math.reduce_any(next_indices):\n",
    "            break\n",
    "\n",
    "        selected.append(tf.argsort(\n",
    "            tf.dtypes.cast(next_indices, tf.int32), direction='DESCENDING')[0].numpy())\n",
    "\n",
    "    return tf.gather(idx, selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_predict(predictions, priors, cfg):\n",
    "    label_classes = cfg['labels_list']\n",
    "\n",
    "    bbox_regressions, confs = tf.split(predictions[0], [4, -1], axis=-1)\n",
    "    boxes = decode_bbox_tf(bbox_regressions, priors, cfg['variances'])\n",
    "\n",
    "\n",
    "    confs = tf.math.softmax(confs, axis=-1)\n",
    "\n",
    "    out_boxes = []\n",
    "    out_labels = []\n",
    "    out_scores = []\n",
    "\n",
    "    for c in range(1, len(label_classes)):\n",
    "        cls_scores = confs[:, c]\n",
    "        \n",
    "        score_idx = cls_scores > cfg['score_threshold']\n",
    "\n",
    "        cls_boxes = boxes[score_idx]\n",
    "        cls_scores = cls_scores[score_idx]\n",
    "\n",
    "        nms_idx = compute_nms(cls_boxes, cls_scores, cfg['nms_threshold'], cfg['max_number_keep'])\n",
    "\n",
    "        cls_boxes = tf.gather(cls_boxes, nms_idx)\n",
    "        cls_scores = tf.gather(cls_scores, nms_idx)\n",
    "\n",
    "        cls_labels = [c] * cls_boxes.shape[0]\n",
    "\n",
    "        out_boxes.append(cls_boxes)\n",
    "        out_labels.extend(cls_labels)\n",
    "        out_scores.append(cls_scores)\n",
    "\n",
    "    out_boxes = tf.concat(out_boxes, axis=0)\n",
    "    out_scores = tf.concat(out_scores, axis=0)\n",
    "\n",
    "    boxes = tf.clip_by_value(out_boxes, 0.0, 1.0).numpy()\n",
    "    classes = np.array(out_labels)\n",
    "    scores = out_scores.numpy()\n",
    "\n",
    "    return boxes, classes, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference(2) 사진에서 얼굴 찾기\n",
    "## 사진에서 여러개의 얼굴을 찾아보자.\n",
    "- 이제 다 왔습니다. SSD 모델을 통해 우리는 Multi-face detection 기능을 확보했습니다.\n",
    "- 얼마나 잘 해내는지 확인해 보도록 합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_input_image(img, max_steps):\n",
    "    \"\"\"pad image to suitable shape\"\"\"\n",
    "    img_h, img_w, _ = img.shape\n",
    "\n",
    "    img_pad_h = 0\n",
    "    if img_h % max_steps > 0:\n",
    "        img_pad_h = max_steps - img_h % max_steps\n",
    "\n",
    "    img_pad_w = 0\n",
    "    if img_w % max_steps > 0:\n",
    "        img_pad_w = max_steps - img_w % max_steps\n",
    "\n",
    "    padd_val = np.mean(img, axis=(0, 1)).astype(np.uint8)\n",
    "    img = cv2.copyMakeBorder(img, 0, img_pad_h, 0, img_pad_w,\n",
    "                             cv2.BORDER_CONSTANT, value=padd_val.tolist())\n",
    "    pad_params = (img_h, img_w, img_pad_h, img_pad_w)\n",
    "\n",
    "    return img, pad_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_pad_output(outputs, pad_params):\n",
    "    \"\"\"\n",
    "        recover the padded output effect\n",
    "\n",
    "    \"\"\"\n",
    "    img_h, img_w, img_pad_h, img_pad_w = pad_params\n",
    "\n",
    "    recover_xy = np.reshape(outputs[0], [-1, 2, 2]) * \\\n",
    "                 [(img_pad_w + img_w) / img_w, (img_pad_h + img_h) / img_h]\n",
    "    outputs[0] = np.reshape(recover_xy, [-1, 4])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, boxes, classes, scores, img_height, img_width, prior_index, class_list):\n",
    "    \"\"\"\n",
    "    draw bboxes and labels\n",
    "    out:boxes,classes,scores\n",
    "    \"\"\"\n",
    "    # bbox\n",
    "\n",
    "    x1, y1, x2, y2 = int(boxes[prior_index][0] * img_width), int(boxes[prior_index][1] * img_height), \\\n",
    "                     int(boxes[prior_index][2] * img_width), int(boxes[prior_index][3] * img_height)\n",
    "    if classes[prior_index] == 1:\n",
    "        color = (0, 255, 0)\n",
    "    else:\n",
    "        color = (0, 0, 255)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "    # confidence\n",
    "\n",
    "    # if scores:\n",
    "    score = \"{:.4f}\".format(scores[prior_index])\n",
    "    class_name = class_list[classes[prior_index]]\n",
    "\n",
    "    cv2.putText(img, '{} {}'.format(class_name, score),\n",
    "                (int(boxes[prior_index][0] * img_width), int(boxes[prior_index][1] * img_height) - 4),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- inference와 화면출력을 위한 몇가지 기능을 추가하였습니다.\n",
    "\n",
    "- 여러 사람의 얼굴이 포함된 테스트용 이미지를 골라 주세요. ~/aiffel/face_detector/image.png라는 경로로 저장 후 아래 코드를 실행해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import os, sys\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import config \n",
    "from make_prior_box import prior_box\n",
    "from tf_dataloader import load_dataset, _jaccard \n",
    "from tf_build_ssd_model import SsdModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# hyperparameters\n",
    "# args = argparse.ArgumentParser()\n",
    "# args.add_argument('model_path', type=str, nargs='?', default='checkpoints/')\n",
    "# args.add_argument('img_path', type=str, nargs='?', default='images/')\n",
    "# args.add_argument('camera', type=str, nargs='?', default=False)\n",
    "\n",
    "# args_config = args.parse_args()\n",
    "# args_config = \"images/image.png\"\n",
    "\n",
    "global model\n",
    "cfg = config.cfg\n",
    "min_sizes = cfg['min_sizes']\n",
    "num_cell = [len(min_sizes[k]) for k in range(len(cfg['steps']))]\n",
    "model_path = os.getenv('HOME')+'/aiffel/face_detector/checkpoints'\n",
    "img_path = os.getenv('HOME')+'/aiffel/face_detector/image.png'\n",
    "\n",
    "try:\n",
    "    model = SsdModel(cfg=cfg, num_cell=num_cell, training=False)\n",
    "\n",
    "    paths = [os.path.join(model_path, path)\n",
    "             for path in os.listdir(model_path)]\n",
    "    latest = sorted(paths, key=os.path.getmtime)[-1]\n",
    "    model.load_weights(latest)\n",
    "    print(f\"[*] model path : {latest}\")\n",
    "\n",
    "except AttributeError as e:\n",
    "    print('Please make sure there is at least one weights at {}'.format(model_path))\n",
    "\n",
    "if not os.path.exists(img_path):\n",
    "    print(f\"Cannot find image path from {img_path}\")\n",
    "    exit()\n",
    "    \n",
    "print(\"[*] Predict {} image.. \".format(img_path))\n",
    "img_raw = cv2.imread(img_path)\n",
    "img_raw = cv2.resize(img_raw, (384, 256))\n",
    "img_height_raw, img_width_raw, _ = img_raw.shape\n",
    "img = np.float32(img_raw.copy())\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# pad input image to avoid unmatched shape problem\n",
    "img, pad_params = pad_input_image(img, max_steps=max(cfg['steps']))\n",
    "img = img / 255.0 - 0.5\n",
    "\n",
    "print(f\"[*] image.shape:{img.shape}\")\n",
    "\n",
    "priors, _ = prior_box(cfg, image_sizes=(img.shape[0], img.shape[1]))\n",
    "priors = tf.cast(priors, tf.float32)\n",
    "\n",
    "predictions = model.predict(img[np.newaxis, ...])\n",
    "\n",
    "boxes, classes, scores = parse_predict(predictions, priors, cfg)\n",
    "\n",
    "print(f\"[*] scores:{scores}\")\n",
    "\n",
    "# recover padding effect\n",
    "boxes = recover_pad_output(boxes, pad_params)\n",
    "\n",
    "print(f\"[*] boxes:{boxes}\")\n",
    "\n",
    "# draw and save results\n",
    "# save_img_path = os.path.join('assets/out_' + img_path)\n",
    "save_img_path = os.getenv('HOME')+'/aiffel/face_detector/assets/out_image.png'\n",
    "print(f\"[*] save_img_path:{save_img_path}\")\n",
    "\n",
    "for prior_index in range(len(boxes)):\n",
    "    score = \"{:.4f}\".format(scores[prior_index])\n",
    "    class_name = cfg['labels_list']\n",
    "    show_image(img_raw, boxes, classes, scores, img_height_raw, img_width_raw, prior_index, cfg['labels_list'])\n",
    "\n",
    "cv2.imwrite(save_img_path, img_raw)\n",
    "cv2.imshow('results', img_raw)\n",
    "# if cv2.waitKey(0) == ord('q'):\n",
    "#     exit(0)\n",
    "if cv2.waitKey(100) and 0xFF == ord('q'):\n",
    "    exit(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 스텝에 소개한 모델 학습 과정을 inference.py에 정리해 두었습니다. 아래와 같이 위의 과정을 실행할 수 있습니다.\n",
    "(단, 이미지 파일명은 임의로 변경할 수 있습니다. )\n",
    "\n",
    "- $ cd ~/aiffel/face_detector && python inference.py  checkpoints/image.png \n",
    "어떤가요? 결과가 잘 나오시나요? 저는 이렇게 결과가 나왔습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img_path = os.getenv('HOME')+'/aiffel/face_detector/image.png'\n",
    "\n",
    "def to_yolo_convert(size, box):\n",
    "    dw = 1./size[0]\n",
    "    dh = 1./size[1]\n",
    "    x = round((box[0] + box[1])/2.0)\n",
    "    y = round((box[2] + box[3])/2.0)\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    x = x*dw\n",
    "    w = w*dw\n",
    "    y = y*dh\n",
    "    h = h*dh\n",
    "    return (x,w,y,h)\n",
    "\n",
    "img_raw = cv2.imread(img_path)\n",
    "img_raw = cv2.resize(img_raw, (384, 256))\n",
    "img_height_raw, img_width_raw, _ = img_raw.shape\n",
    "img = np.float32(img_raw.copy())\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# pad input image to avoid unmatched shape problem\n",
    "img, pad_params = pad_input_image(img, max_steps=max(cfg['steps']))\n",
    "img = img / 255.0 - 0.5\n",
    "\n",
    "print(f\"[*] image.shape:{img.shape}\")\n",
    "\n",
    "w= img.shape[0]\n",
    "h= img.shape[1]\n",
    "print(w,h)\n",
    "\n",
    "xmin = 213\n",
    "ymin = 108\n",
    "xmax = 256\n",
    "ymax = 151\n",
    "\n",
    "print(xmin, xmax, ymin, ymax) #define your x,y coordinates\n",
    "b = (xmin, xmax, ymin, ymax)\n",
    "bb = to_yolo_convert((w,h), b)\n",
    "print(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img_path = os.getenv('HOME')+'/aiffel/face_detector/image.png'\n",
    "\n",
    "def from_yolo_convert(size, box):\n",
    "    dw = size[0]\n",
    "    dh = size[1]\n",
    "    x = box[0]*dw\n",
    "    w = box[1]*dw\n",
    "    y = box[2]*dh\n",
    "    h = box[3]*dh\n",
    "    xmin = round(x - w // 2)\n",
    "    ymin = round(y - h // 2)\n",
    "    xmax = round(x + w // 2)    \n",
    "    ymax = round(y + h // 2)\n",
    "    return (xmin,ymin,xmax,ymax)\n",
    "\n",
    "img_raw = cv2.imread(img_path)\n",
    "img_raw = cv2.resize(img_raw, (384, 256))\n",
    "img_height_raw, img_width_raw, _ = img_raw.shape\n",
    "img = np.float32(img_raw.copy())\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# pad input image to avoid unmatched shape problem\n",
    "img, pad_params = pad_input_image(img, max_steps=max(cfg['steps']))\n",
    "img = img / 255.0 - 0.5\n",
    "\n",
    "print(f\"[*] image.shape:{img.shape}\")\n",
    "\n",
    "w= img.shape[0]\n",
    "h= img.shape[1]\n",
    "print(w,h)\n",
    "\n",
    "xcen = 0.7197022\n",
    "xwidth = 0.21654367\n",
    "ycen = 0.8512748\n",
    "yheight = 0.41132832\n",
    "\n",
    "print(xcen, xwidth, ycen, yheight) #define your x,y coordinates\n",
    "b = (xcen, xwidth, ycen, yheight)\n",
    "bb = from_yolo_convert((w,h), b)\n",
    "print(bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. 스티커 구하기 혹은 만들기\n",
    "- 여러분들은 이미 [카메라 스티커앱 만들기 첫걸음] 노드를 수행해 보셨을 것입니다. 왕관 또는 고양이 수염, 혹은 다양한 아이디어의 스티커를 만들어 볼 수 있을 것입니다. 당시의 스티커 이미지를 그대로 활용하셔도 되고, 다른 이미지를 사용하셔도 무방합니다.\n",
    "\n",
    "# Step 2. SSD 모델을 통해 얼굴 bounding box 찾기\n",
    "- 우리는 실습코드를 진행하며 필요한 모델을 이미 생성해 왔을 것입니다. 잘 훈련된 해당 모델을 통해 적절한 얼굴 bounding box를 찾아내 봅시다. inference.py 코드를 적극 참고해 보시기를 권합니다.\n",
    "\n",
    "# Step 3. dlib 을 이용한 landmark 찾기\n",
    "- 검출된 bounding box별로 dlib을 이용해 face landmark를 찾을 수 있을 것입니다. inference.py 에서 show_image 메소드를 사용한 부분을 잘 수정하면 가능할 것입니다.\n",
    "\n",
    "# Step 4. 스티커 합성 사진 생성하기\n",
    "- 여러분들이 선택한 인물사진에 스티커를 합성해 봅시다. 이미지에 너무 많은 사람 얼굴이 포함되어 있거나, 검출된 얼굴이 너무 작아서 스티커 합성이 어울리지 않으면 적당하지 않겠죠? 3~5명 정도의 얼굴이 포함된 적당한 사진을 선택해 주세요.\n",
    "- 1명의 얼굴에 스티커를 붙여주는 방법은 이미 알고 계실 것입니다. 생성된 이미지를 프로젝트 코드와 함께 제출해 주세요~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.getenv('HOME')+'/aiffel/face_detector/image.png'\n",
    "\n",
    "img_bgr = cv2.imread(img_path)\n",
    "img_bgr = cv2.resize(img_bgr, dsize=(384,256)) # 편의를 위해 이미지 크기를 변경합니다. 640x360의 VGA 크기(16:9)로 고정\n",
    "print(img_bgr.shape)\n",
    "img_show = img_bgr.copy() # 출력용 이미지 복사\n",
    "plt.imshow(img_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow 이전에 RGB 이미지로 바꾸는 것을 잊지마세요. \n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlib을 활용해 hog detector를 선언해 봅시다\n",
    "import dlib\n",
    "detector_hog = dlib.get_frontal_face_detector()   # detector 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector를 이용해서 얼굴의 bounding box를 추출\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "dlib_rects = detector_hog(img_rgb, 1)   # (image, num of img pyramid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 찾은 얼굴 화면에 출력하기\n",
    "\n",
    "for dlib_rect in dlib_rects:\n",
    "    l = dlib_rect.left()\n",
    "    t = dlib_rect.top()\n",
    "    r = dlib_rect.right()\n",
    "    b = dlib_rect.bottom()\n",
    "    \n",
    "    print(dlib_rect, l, t, r, b)\n",
    "    cv2.rectangle(img_rgb, (l,t), (r,b), (0,255,0), 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "plt.imshow(img_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_predictor = dlib.shape_predictor('./models/shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmark_predictor 는 RGB 이미지와 dlib.rectangle을 입력 받아 dlib.full_object_detection 를 반환합니다.\n",
    "\n",
    "list_landmarks = []\n",
    "for dlib_rect in dlib_rects:\n",
    "    points = landmark_predictor(img_rgb, dlib_rect)\n",
    "    list_points = list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "    list_landmarks.append(list_points)\n",
    "\n",
    "i = 0\n",
    "for dlib_rect in dlib_rects:\n",
    "    print(i, len(list_landmarks[i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_landmarks = []\n",
    "for dlib_rect in dlib_rects:\n",
    "    points = landmark_predictor(img_rgb, dlib_rect)\n",
    "    list_points = list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "    list_landmarks.append(list_points)\n",
    "    for landmark in list_landmarks:\n",
    "        for idx, point in enumerate(list_points):\n",
    "            cv2.circle(img_show, point, 2, (0, 255, 255), -1) # yellow\n",
    "\n",
    "img_rgb = cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 좌표 확인해 보기\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "w = []\n",
    "h = []\n",
    "i = 0\n",
    "for dlib_rect, landmark in zip(dlib_rects, list_landmarks):\n",
    "    print (landmark[20]) \n",
    "    print (landmark[25]) \n",
    "    x.append((landmark[20][0]+landmark[25][0])//2)\n",
    "    y.append((landmark[20][1]+landmark[25][1])//2 - dlib_rect.width()//2)\n",
    "    w.append(dlib_rect.width())\n",
    "    h.append(dlib_rect.height())\n",
    "    print ('(x,y) : (%d,%d)'%(x[i],y[i]))\n",
    "    print ('(w,h) : (%d,%d)'%(w[i],h[i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 원본 이미지에 스티커 이미지를 추가하기 위해서 x, y 좌표를 조정합니다. 이미지 시작점은 top-left 좌표이기 때문입니다.\n",
    "\n",
    "refined_x = []\n",
    "refined_y = []\n",
    "i = 0\n",
    "for dlib_rect in dlib_rects:\n",
    "    print ('(x,y) : (%d,%d)'%(x[i], y[i]))\n",
    "    print ('(w,h) : (%d,%d)'%(w[i], h[i]))\n",
    "    refined_x.append(x[i] - w[i] // 2)  # left\n",
    "    refined_y.append(y[i] - h[i]) # top\n",
    "    print ('(refined_x,refined_y) : (%d,%d)'%(refined_x[i], refined_y[i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refined_y = 0\n",
    "print ('(x,y) : (%d,%d)'%(refined_x[1], refined_y[1])) # landmark_predictor 는 RGB 이미지와 dlib.rectangle을 입력 받아 dlib.full_object_detection 를 반환합니다.\n",
    "\n",
    "list_landmarks = []\n",
    "for dlib_rect in dlib_rects:\n",
    "    points = landmark_predictor(img_rgb, dlib_rect)\n",
    "    list_points = list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "    list_landmarks.append(list_points)\n",
    "\n",
    "print(len(list_landmarks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 이미지에 스티커를 적용합니다. \n",
    "img_sticker = cv2.imread('/home/aiffel0042/aiffel/camera_sticker/images/yellow.png')\n",
    "sticker_area = []\n",
    "i = 0\n",
    "for dlib_rect in dlib_rects:\n",
    "    img_sticker = cv2.resize(img_sticker, (w[i],h[i]))\n",
    "    if refined_x[i] < 0 :\n",
    "        if refined_y[i] < 0 :\n",
    "            img_sticker = img_sticker[-refined_y[i]:-refined_x[i]]    \n",
    "            sticker_area.append(img_show[-refined_y[i]:-refined_y[i]+img_sticker.shape[0], -refined_x[i]:-refined_x[i]+img_sticker.shape[1]])                            \n",
    "            img_show[-refined_y[i]:-refined_y[i]+img_sticker.shape[0], -refined_x[i]:-refined_x[i]+img_sticker.shape[1]] = \\\n",
    "                np.where(img_sticker==0,sticker_area[i],img_sticker).astype(np.uint8)\n",
    "        else :\n",
    "            img_sticker = img_sticker[:-refined_x[i]]\n",
    "            sticker_area.append(img_show[refined_y[i]:refined_y[i]+img_sticker.shape[0], -refined_x[i]:-refined_x[i]+img_sticker.shape[1]])                            \n",
    "            img_show[refined_y[i]:refined_y[i]+img_sticker.shape[0], -refined_x[i]:-refined_x[i]+img_sticker.shape[1]] = \\\n",
    "                np.where(img_sticker==0,sticker_area[i],img_sticker).astype(np.uint8)\n",
    "\n",
    "    else :\n",
    "        if refined_y[i] < 0 :\n",
    "            img_sticker = img_sticker[-refined_y[i]:]\n",
    "            sticker_area.append(img_show[-refined_y[i]:-refined_y[i]+img_sticker.shape[0], refined_x[i]:refined_x[i]+img_sticker.shape[1]])                            \n",
    "            img_show[-refined_y[i]:-refined_y[i]+img_sticker.shape[0], refined_x[i]:refined_x[i]+img_sticker.shape[1]] = \\\n",
    "                np.where(img_sticker==0,sticker_area[i],img_sticker).astype(np.uint8)\n",
    "\n",
    "        else :\n",
    "            img_sticker = img_sticker[:]\n",
    "            sticker_area.append(img_show[refined_y[i]:refined_y[i]+img_sticker.shape[0], refined_x[i]:refined_x[i]+img_sticker.shape[1]])                            \n",
    "            img_show[refined_y[i]:refined_y[i]+img_sticker.shape[0], refined_x[i]:refined_x[i]+img_sticker.shape[1]] = \\\n",
    "                np.where(img_sticker==0,sticker_area[i],img_sticker).astype(np.uint8)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sticker = cv2.imread('/home/aiffel0042/aiffel/camera_sticker/images/yellow.png')\n",
    "sticker_area = []\n",
    "i = 0\n",
    "for dlib_rect in dlib_rects:\n",
    "    img_sticker = cv2.resize(img_sticker, (w[i],h[i]))\n",
    "    if refined_x[i] < 0 :\n",
    "        if refined_y[i] < 0 :\n",
    "            img_sticker = img_sticker[-refined_y[i]:-refined_x[i]]\n",
    "            sticker_area.append(img_bgr[-refined_y[i]:-refined_y[i]+img_sticker.shape[0], -refined_x[i]:-refined_x[i]+img_sticker.shape[1]])                            \n",
    "            img_bgr[-refined_y[i]:-refined_y[i]+img_sticker.shape[0], -refined_x[i]:-refined_x[i]+img_sticker.shape[1]] = \\\n",
    "                np.where(img_sticker==0,sticker_area[i],img_sticker).astype(np.uint8)\n",
    "        else :\n",
    "            img_sticker = img_sticker[:-refined_x[i]]\n",
    "            sticker_area.append(img_bgr[refined_y[i]:refined_y[i]+img_sticker.shape[0], -refined_x[i]:-refined_x[i]+img_sticker.shape[1]])                            \n",
    "            img_bgr[refined_y[i]:refined_y[i]+img_sticker.shape[0], -refined_x[i]:-refined_x[i]+img_sticker.shape[1]] = \\\n",
    "                np.where(img_sticker==0,sticker_area[i],img_sticker).astype(np.uint8)\n",
    "\n",
    "    else :\n",
    "        if refined_y[i] < 0 :\n",
    "            img_sticker = img_sticker[-refined_y[i]:]\n",
    "            sticker_area.append(img_bgr[-refined_y[i]:-refined_y[i]+img_sticker.shape[0], refined_x[i]:refined_x[i]+img_sticker.shape[1]])                            \n",
    "            img_bgr[-refined_y[i]:-refined_y[i]+img_sticker.shape[0], refined_x[i]:refined_x[i]+img_sticker.shape[1]] = \\\n",
    "                np.where(img_sticker==0,sticker_area[i],img_sticker).astype(np.uint8)\n",
    "\n",
    "        else :\n",
    "            img_sticker = img_sticker[:]\n",
    "            sticker_area.append(img_bgr[refined_y[i]:refined_y[i]+img_sticker.shape[0], refined_x[i]:refined_x[i]+img_sticker.shape[1]])                            \n",
    "            img_bgr[refined_y[i]:refined_y[i]+img_sticker.shape[0], refined_x[i]:refined_x[i]+img_sticker.shape[1]] = \\\n",
    "                np.where(img_sticker==0,sticker_area[i],img_sticker).astype(np.uint8)\n",
    "    i += 1\n",
    "    \n",
    "plt.imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
